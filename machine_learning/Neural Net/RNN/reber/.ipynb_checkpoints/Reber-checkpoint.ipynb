{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REBER Grammar with RNN\n",
    "\n",
    "In this workbook, we are going to set-up multiple Recurrent Neural Network to test them using as test <a href=\"https://www.willamette.edu/~gorr/classes/cs449/reber.html\" target=\"_blank\">Reber's grammar</a> words.\n",
    "\n",
    "## What is a Reber Word ?\n",
    "\n",
    "A Reber word is a word following the Reber's grammar. The grammar is based on the following graph:\n",
    "\n",
    "<img src=\"reber.gif\"/>\n",
    "\n",
    "The word must start with B, then it can be either T or P and so on until it reaches E. To prepare data for this, we are going to use a OneHotEncoder to have 7 inputs, n timesteps (depending on the length of the word) and k batches. To generate it, I use the algorith from <a href=\"http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/reberGrammar.php\" target=\"_target\">this site</a>\n",
    "\n",
    "It also exists an Embedded version of the Reber Grammar using the following graph :\n",
    "\n",
    "<img src=\"embreber.gif\"/>\n",
    "\n",
    "For now, we gonna focus on the simple version and based on the result, we may try the embedded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "import create_dataset as reber\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of datas\n",
    "\n",
    "For the OneHotEncoder, the chain 'BTSXPVE' will be used. We can now try only 1 example to check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTSXXTTVPXVV\n",
      "[ 1.  0.  0.  0.  0.  0.  0.] [ 0.  1.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "x, y = reber.get_one_example(minLength=10)\n",
    "print(reber.sequenceToWord(x))\n",
    "print(x[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*y* is the possible outcome for a given input. That means B ([ 1.  0.  0.  0.  0.  0.  0.]) can be followed by T or P ([ 0.  1.  0.  0.  1.  0.  0.]).\n",
    "\n",
    "However, we won't use y as output but for every timestep, we are going to provide the next timestep as target. For this, we will use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(x0):\n",
    "    end = np.array([0.,  0.,  0.,  0.,  0.,  0.,  1.])\n",
    "    y=x0[1:]\n",
    "    y.append(end)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take as input \"BTSXS\", the output will be \"TSXSE\" (but the input in encoded).\n",
    "\n",
    "We can also generate few words to check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTSSSXXTTTVV\n",
      "BTXXTTTTVV\n",
      "BTSSSXXTTTVV\n",
      "BTXXVPXVPXTVV\n",
      "BPTVPXVPXTVV\n",
      "BTSXXVPXVV\n",
      "BPTTTVPXTTTTVV\n",
      "BPTTTTTTTVV\n",
      "BTSSXXTTVV\n",
      "BTXXTTVPXVV\n"
     ]
    }
   ],
   "source": [
    "min_length = 10\n",
    "for i in range(10):\n",
    "    inp, out = reber.get_one_example(min_length)\n",
    "    print(reber.sequenceToWord(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the first \"problem\" now, the length of the string is variable. So when we are going to generate our test/train datas, we will have to pad them to the same length (let's say 20). This is done by using <b>sequence.pad_sequences</b> for Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 20, 7)\n",
      "(2048, 20, 7)\n",
      "(256, 20, 7)\n",
      "(256, 20, 7)\n",
      "(1, 20, 7)\n",
      "(1, 20, 7)\n",
      "(1, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "X_val, y_val = [], []\n",
    "y_possible = []\n",
    "\n",
    "for i in range(2048):\n",
    "    x, y = reber.get_one_example(min_length)\n",
    "    X_train.append(x)\n",
    "    y_train.append(generate(x))\n",
    "\n",
    "for i in range(256):\n",
    "    x, y = reber.get_one_example(min_length)\n",
    "    X_test.append(x)\n",
    "    y_test.append(generate(x))  \n",
    "    \n",
    "for i in range(1):\n",
    "    x, y = reber.get_one_example(min_length)\n",
    "    X_val.append(x)\n",
    "    y_val.append(generate(x))\n",
    "    y_possible.append(y)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "y_possible = np.array(y_possible)\n",
    "\n",
    "maxlen = 20\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_train = sequence.pad_sequences(y_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_test = sequence.pad_sequences(y_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_val = sequence.pad_sequences(y_val, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_possible = sequence.pad_sequences(y_possible, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_possible.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1 0 0 1 0 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 1 0 0 0 1 0]\n",
      "  [0 0 0 0 1 1 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 1 0 0 0 1 0]\n",
      "  [0 1 0 0 0 1 0]\n",
      "  [0 1 0 0 0 1 0]\n",
      "  [0 0 0 0 1 1 0]\n",
      "  [0 0 1 1 0 0 0]\n",
      "  [0 1 0 0 0 1 0]\n",
      "  [0 0 0 0 1 1 0]\n",
      "  [0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have 2048 strings for training, 256 for test and 1 just for visualisation later. We can now set-up our model.\n",
    "\n",
    "## Test of RNNs\n",
    "\n",
    "For this model, we are going to use a many-to-many RNN. That means for every input, the model will predict an output. The training will be done based on the input we prepared previously. Once trained. We will be able to \"transfer\" the learning to a one-to-many model in order to have a \"generator\".\n",
    "\n",
    "<img src=\"RNN_types.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the writing of this notebook, I tried some loss, metrics and optimizer. The following ones are the one fitting the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_unit = 7\n",
    "inp_shape = (maxlen, 7)\n",
    "loss_ = \"mean_squared_error\"\n",
    "metrics_ = \"mean_squared_error\"\n",
    "optimizer_ = \"Nadam\"\n",
    "nb_epoch = 250\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "The first model we will setup is an <b>LSTM</b> which means <b>L</b>ong <b>S</b>hort-<b>T</b>erm <b>M</b>emory. The principle is \n",
    "quite complex but very powerfull for long sequences inputs (because there is less issues with Vanishing Gradient Problem) or long term memory (You can refer to <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">this link</a> for more informations)\n",
    "\n",
    "LSTM is widely for speech recognition, Natural Language processing, Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=nb_unit, input_shape=inp_shape, return_sequences=True))  # single LSTM\n",
    "model.compile(loss=loss_,\n",
    "              optimizer=optimizer_,\n",
    "              metrics=[metrics_])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"lstm_simple.h5\",\n",
    "    monitor=loss_,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1)\n",
    "early = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: (None, 20, 7)\n",
      "Outputs: (None, 20, 7)\n",
      "Actual input: (2048, 20, 7)\n",
      "Actual output: (2048, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs: {}\".format(model.input_shape))\n",
    "print(\"Outputs: {}\".format(model.output_shape))\n",
    "print(\"Actual input: {}\".format(X_train.shape))\n",
    "print(\"Actual output: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 256 samples\n",
      "Epoch 1/250\n",
      "Epoch 00001: mean_squared_error improved from inf to 0.08666, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0867 - mean_squared_error: 0.0867 - val_loss: 0.0810 - val_mean_squared_error: 0.0810\n",
      "Epoch 2/250\n",
      "Epoch 00002: mean_squared_error improved from 0.08666 to 0.07840, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.0751 - val_mean_squared_error: 0.0751\n",
      "Epoch 3/250\n",
      "Epoch 00003: mean_squared_error improved from 0.07840 to 0.07357, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0736 - mean_squared_error: 0.0736 - val_loss: 0.0711 - val_mean_squared_error: 0.0711\n",
      "Epoch 4/250\n",
      "Epoch 00004: mean_squared_error improved from 0.07357 to 0.06978, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 5/250\n",
      "Epoch 00005: mean_squared_error improved from 0.06978 to 0.06628, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0663 - mean_squared_error: 0.0663 - val_loss: 0.0642 - val_mean_squared_error: 0.0642\n",
      "Epoch 6/250\n",
      "Epoch 00006: mean_squared_error improved from 0.06628 to 0.06294, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0629 - mean_squared_error: 0.0629 - val_loss: 0.0609 - val_mean_squared_error: 0.0609\n",
      "Epoch 7/250\n",
      "Epoch 00007: mean_squared_error improved from 0.06294 to 0.05971, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0597 - mean_squared_error: 0.0597 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 8/250\n",
      "Epoch 00008: mean_squared_error improved from 0.05971 to 0.05663, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0566 - mean_squared_error: 0.0566 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 9/250\n",
      "Epoch 00009: mean_squared_error improved from 0.05663 to 0.05375, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0537 - mean_squared_error: 0.0537 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 10/250\n",
      "Epoch 00010: mean_squared_error improved from 0.05375 to 0.05119, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 11/250\n",
      "Epoch 00011: mean_squared_error improved from 0.05119 to 0.04905, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 12/250\n",
      "Epoch 00012: mean_squared_error improved from 0.04905 to 0.04729, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 13/250\n",
      "Epoch 00013: mean_squared_error improved from 0.04729 to 0.04582, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 14/250\n",
      "Epoch 00014: mean_squared_error improved from 0.04582 to 0.04458, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 15/250\n",
      "Epoch 00015: mean_squared_error improved from 0.04458 to 0.04360, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 16/250\n",
      "Epoch 00016: mean_squared_error improved from 0.04360 to 0.04278, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 17/250\n",
      "Epoch 00017: mean_squared_error improved from 0.04278 to 0.04209, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 18/250\n",
      "Epoch 00018: mean_squared_error improved from 0.04209 to 0.04147, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 19/250\n",
      "Epoch 00019: mean_squared_error improved from 0.04147 to 0.04093, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 20/250\n",
      "Epoch 00020: mean_squared_error improved from 0.04093 to 0.04046, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 21/250\n",
      "Epoch 00021: mean_squared_error improved from 0.04046 to 0.04004, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 22/250\n",
      "Epoch 00022: mean_squared_error improved from 0.04004 to 0.03965, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 23/250\n",
      "Epoch 00023: mean_squared_error improved from 0.03965 to 0.03931, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 24/250\n",
      "Epoch 00024: mean_squared_error improved from 0.03931 to 0.03900, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 25/250\n",
      "Epoch 00025: mean_squared_error improved from 0.03900 to 0.03873, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 26/250\n",
      "Epoch 00026: mean_squared_error improved from 0.03873 to 0.03848, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 27/250\n",
      "Epoch 00027: mean_squared_error improved from 0.03848 to 0.03826, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 28/250\n",
      "Epoch 00028: mean_squared_error improved from 0.03826 to 0.03807, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 29/250\n",
      "Epoch 00029: mean_squared_error improved from 0.03807 to 0.03790, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 30/250\n",
      "Epoch 00030: mean_squared_error improved from 0.03790 to 0.03775, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 31/250\n",
      "Epoch 00031: mean_squared_error improved from 0.03775 to 0.03761, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 32/250\n",
      "Epoch 00032: mean_squared_error improved from 0.03761 to 0.03748, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 33/250\n",
      "Epoch 00033: mean_squared_error improved from 0.03748 to 0.03737, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 34/250\n",
      "Epoch 00034: mean_squared_error improved from 0.03737 to 0.03726, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 35/250\n",
      "Epoch 00035: mean_squared_error improved from 0.03726 to 0.03717, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 36/250\n",
      "Epoch 00036: mean_squared_error improved from 0.03717 to 0.03708, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 37/250\n",
      "Epoch 00037: mean_squared_error improved from 0.03708 to 0.03700, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 38/250\n",
      "Epoch 00038: mean_squared_error improved from 0.03700 to 0.03693, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 39/250\n",
      "Epoch 00039: mean_squared_error improved from 0.03693 to 0.03685, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/250\n",
      "Epoch 00040: mean_squared_error improved from 0.03685 to 0.03678, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 41/250\n",
      "Epoch 00041: mean_squared_error improved from 0.03678 to 0.03671, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 42/250\n",
      "Epoch 00042: mean_squared_error improved from 0.03671 to 0.03666, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 43/250\n",
      "Epoch 00043: mean_squared_error improved from 0.03666 to 0.03659, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 44/250\n",
      "Epoch 00044: mean_squared_error improved from 0.03659 to 0.03653, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 45/250\n",
      "Epoch 00045: mean_squared_error improved from 0.03653 to 0.03648, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 46/250\n",
      "Epoch 00046: mean_squared_error improved from 0.03648 to 0.03643, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 47/250\n",
      "Epoch 00047: mean_squared_error improved from 0.03643 to 0.03639, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 48/250\n",
      "Epoch 00048: mean_squared_error improved from 0.03639 to 0.03633, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 49/250\n",
      "Epoch 00049: mean_squared_error improved from 0.03633 to 0.03629, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 50/250\n",
      "Epoch 00050: mean_squared_error improved from 0.03629 to 0.03625, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 51/250\n",
      "Epoch 00051: mean_squared_error improved from 0.03625 to 0.03620, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 52/250\n",
      "Epoch 00052: mean_squared_error improved from 0.03620 to 0.03616, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 53/250\n",
      "Epoch 00053: mean_squared_error improved from 0.03616 to 0.03613, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 54/250\n",
      "Epoch 00054: mean_squared_error improved from 0.03613 to 0.03609, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 55/250\n",
      "Epoch 00055: mean_squared_error improved from 0.03609 to 0.03606, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 56/250\n",
      "Epoch 00056: mean_squared_error improved from 0.03606 to 0.03604, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 57/250\n",
      "Epoch 00057: mean_squared_error improved from 0.03604 to 0.03599, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 58/250\n",
      "Epoch 00058: mean_squared_error improved from 0.03599 to 0.03596, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 59/250\n",
      "Epoch 00059: mean_squared_error improved from 0.03596 to 0.03594, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 60/250\n",
      "Epoch 00060: mean_squared_error improved from 0.03594 to 0.03591, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 61/250\n",
      "Epoch 00061: mean_squared_error improved from 0.03591 to 0.03587, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 62/250\n",
      "Epoch 00062: mean_squared_error improved from 0.03587 to 0.03585, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 63/250\n",
      "Epoch 00063: mean_squared_error improved from 0.03585 to 0.03584, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 64/250\n",
      "Epoch 00064: mean_squared_error improved from 0.03584 to 0.03579, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 65/250\n",
      "Epoch 00065: mean_squared_error improved from 0.03579 to 0.03576, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 66/250\n",
      "Epoch 00066: mean_squared_error improved from 0.03576 to 0.03573, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 67/250\n",
      "Epoch 00067: mean_squared_error improved from 0.03573 to 0.03570, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 68/250\n",
      "Epoch 00068: mean_squared_error improved from 0.03570 to 0.03565, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 69/250\n",
      "Epoch 00069: mean_squared_error improved from 0.03565 to 0.03563, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 70/250\n",
      "Epoch 00070: mean_squared_error improved from 0.03563 to 0.03559, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 71/250\n",
      "Epoch 00071: mean_squared_error improved from 0.03559 to 0.03556, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 72/250\n",
      "Epoch 00072: mean_squared_error improved from 0.03556 to 0.03553, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 73/250\n",
      "Epoch 00073: mean_squared_error improved from 0.03553 to 0.03551, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 74/250\n",
      "Epoch 00074: mean_squared_error improved from 0.03551 to 0.03548, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 75/250\n",
      "Epoch 00075: mean_squared_error improved from 0.03548 to 0.03545, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 76/250\n",
      "Epoch 00076: mean_squared_error improved from 0.03545 to 0.03541, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 77/250\n",
      "Epoch 00077: mean_squared_error improved from 0.03541 to 0.03540, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 78/250\n",
      "Epoch 00078: mean_squared_error improved from 0.03540 to 0.03537, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/250\n",
      "Epoch 00079: mean_squared_error improved from 0.03537 to 0.03533, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 80/250\n",
      "Epoch 00080: mean_squared_error improved from 0.03533 to 0.03531, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 81/250\n",
      "Epoch 00081: mean_squared_error improved from 0.03531 to 0.03529, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 82/250\n",
      "Epoch 00082: mean_squared_error improved from 0.03529 to 0.03526, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 83/250\n",
      "Epoch 00083: mean_squared_error improved from 0.03526 to 0.03522, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 84/250\n",
      "Epoch 00084: mean_squared_error improved from 0.03522 to 0.03521, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 85/250\n",
      "Epoch 00085: mean_squared_error improved from 0.03521 to 0.03519, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 86/250\n",
      "Epoch 00086: mean_squared_error improved from 0.03519 to 0.03515, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 87/250\n",
      "Epoch 00087: mean_squared_error improved from 0.03515 to 0.03512, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 88/250\n",
      "Epoch 00088: mean_squared_error improved from 0.03512 to 0.03510, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 89/250\n",
      "Epoch 00089: mean_squared_error improved from 0.03510 to 0.03507, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 90/250\n",
      "Epoch 00090: mean_squared_error improved from 0.03507 to 0.03504, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 91/250\n",
      "Epoch 00091: mean_squared_error improved from 0.03504 to 0.03502, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 92/250\n",
      "Epoch 00092: mean_squared_error improved from 0.03502 to 0.03500, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 93/250\n",
      "Epoch 00093: mean_squared_error improved from 0.03500 to 0.03496, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 94/250\n",
      "Epoch 00094: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 95/250\n",
      "Epoch 00095: mean_squared_error improved from 0.03496 to 0.03494, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 96/250\n",
      "Epoch 00096: mean_squared_error improved from 0.03494 to 0.03492, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 97/250\n",
      "Epoch 00097: mean_squared_error improved from 0.03492 to 0.03490, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 98/250\n",
      "Epoch 00098: mean_squared_error improved from 0.03490 to 0.03488, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 99/250\n",
      "Epoch 00099: mean_squared_error improved from 0.03488 to 0.03485, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 100/250\n",
      "Epoch 00100: mean_squared_error improved from 0.03485 to 0.03484, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 101/250\n",
      "Epoch 00101: mean_squared_error improved from 0.03484 to 0.03482, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 102/250\n",
      "Epoch 00102: mean_squared_error improved from 0.03482 to 0.03480, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 103/250\n",
      "Epoch 00103: mean_squared_error improved from 0.03480 to 0.03477, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 104/250\n",
      "Epoch 00104: mean_squared_error improved from 0.03477 to 0.03474, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 105/250\n",
      "Epoch 00105: mean_squared_error improved from 0.03474 to 0.03472, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 106/250\n",
      "Epoch 00106: mean_squared_error improved from 0.03472 to 0.03466, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 107/250\n",
      "Epoch 00107: mean_squared_error improved from 0.03466 to 0.03463, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 108/250\n",
      "Epoch 00108: mean_squared_error improved from 0.03463 to 0.03459, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 109/250\n",
      "Epoch 00109: mean_squared_error improved from 0.03459 to 0.03456, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 110/250\n",
      "Epoch 00110: mean_squared_error improved from 0.03456 to 0.03453, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 111/250\n",
      "Epoch 00111: mean_squared_error improved from 0.03453 to 0.03451, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 112/250\n",
      "Epoch 00112: mean_squared_error improved from 0.03451 to 0.03449, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 113/250\n",
      "Epoch 00113: mean_squared_error improved from 0.03449 to 0.03447, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 114/250\n",
      "Epoch 00114: mean_squared_error improved from 0.03447 to 0.03445, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 115/250\n",
      "Epoch 00115: mean_squared_error improved from 0.03445 to 0.03443, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 116/250\n",
      "Epoch 00116: mean_squared_error improved from 0.03443 to 0.03443, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 117/250\n",
      "Epoch 00117: mean_squared_error improved from 0.03443 to 0.03440, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/250\n",
      "Epoch 00118: mean_squared_error improved from 0.03440 to 0.03439, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 119/250\n",
      "Epoch 00119: mean_squared_error improved from 0.03439 to 0.03438, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 120/250\n",
      "Epoch 00120: mean_squared_error improved from 0.03438 to 0.03437, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 121/250\n",
      "Epoch 00121: mean_squared_error improved from 0.03437 to 0.03436, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 122/250\n",
      "Epoch 00122: mean_squared_error improved from 0.03436 to 0.03435, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 123/250\n",
      "Epoch 00123: mean_squared_error improved from 0.03435 to 0.03435, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 124/250\n",
      "Epoch 00124: mean_squared_error improved from 0.03435 to 0.03433, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 125/250\n",
      "Epoch 00125: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 126/250\n",
      "Epoch 00126: mean_squared_error improved from 0.03433 to 0.03431, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 127/250\n",
      "Epoch 00127: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 128/250\n",
      "Epoch 00128: mean_squared_error improved from 0.03431 to 0.03430, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 129/250\n",
      "Epoch 00129: mean_squared_error improved from 0.03430 to 0.03429, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 130/250\n",
      "Epoch 00130: mean_squared_error improved from 0.03429 to 0.03429, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 131/250\n",
      "Epoch 00131: mean_squared_error improved from 0.03429 to 0.03427, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 132/250\n",
      "Epoch 00132: mean_squared_error improved from 0.03427 to 0.03426, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 133/250\n",
      "Epoch 00133: mean_squared_error improved from 0.03426 to 0.03425, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 134/250\n",
      "Epoch 00134: mean_squared_error improved from 0.03425 to 0.03424, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 135/250\n",
      "Epoch 00135: mean_squared_error improved from 0.03424 to 0.03423, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 136/250\n",
      "Epoch 00136: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 137/250\n",
      "Epoch 00137: mean_squared_error improved from 0.03423 to 0.03422, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 138/250\n",
      "Epoch 00138: mean_squared_error improved from 0.03422 to 0.03421, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 139/250\n",
      "Epoch 00139: mean_squared_error improved from 0.03421 to 0.03420, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 140/250\n",
      "Epoch 00140: mean_squared_error improved from 0.03420 to 0.03420, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 141/250\n",
      "Epoch 00141: mean_squared_error improved from 0.03420 to 0.03419, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 142/250\n",
      "Epoch 00142: mean_squared_error improved from 0.03419 to 0.03418, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 143/250\n",
      "Epoch 00143: mean_squared_error improved from 0.03418 to 0.03417, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 144/250\n",
      "Epoch 00144: mean_squared_error improved from 0.03417 to 0.03415, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 145/250\n",
      "Epoch 00145: mean_squared_error improved from 0.03415 to 0.03415, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 146/250\n",
      "Epoch 00146: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 147/250\n",
      "Epoch 00147: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 148/250\n",
      "Epoch 00148: mean_squared_error improved from 0.03415 to 0.03414, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 149/250\n",
      "Epoch 00149: mean_squared_error improved from 0.03414 to 0.03413, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 150/250\n",
      "Epoch 00150: mean_squared_error improved from 0.03413 to 0.03413, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 151/250\n",
      "Epoch 00151: mean_squared_error improved from 0.03413 to 0.03413, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 152/250\n",
      "Epoch 00152: mean_squared_error improved from 0.03413 to 0.03412, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 153/250\n",
      "Epoch 00153: mean_squared_error improved from 0.03412 to 0.03411, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 154/250\n",
      "Epoch 00154: mean_squared_error improved from 0.03411 to 0.03410, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 155/250\n",
      "Epoch 00155: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 156/250\n",
      "Epoch 00156: mean_squared_error improved from 0.03410 to 0.03409, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 157/250\n",
      "Epoch 00157: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 158/250\n",
      "Epoch 00158: mean_squared_error improved from 0.03409 to 0.03409, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/250\n",
      "Epoch 00159: mean_squared_error improved from 0.03409 to 0.03408, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 160/250\n",
      "Epoch 00160: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 161/250\n",
      "Epoch 00161: mean_squared_error improved from 0.03408 to 0.03407, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 162/250\n",
      "Epoch 00162: mean_squared_error improved from 0.03407 to 0.03405, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 163/250\n",
      "Epoch 00163: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 164/250\n",
      "Epoch 00164: mean_squared_error improved from 0.03405 to 0.03404, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 165/250\n",
      "Epoch 00165: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 166/250\n",
      "Epoch 00166: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 167/250\n",
      "Epoch 00167: mean_squared_error improved from 0.03404 to 0.03404, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 168/250\n",
      "Epoch 00168: mean_squared_error improved from 0.03404 to 0.03403, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 169/250\n",
      "Epoch 00169: mean_squared_error improved from 0.03403 to 0.03402, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 170/250\n",
      "Epoch 00170: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 171/250\n",
      "Epoch 00171: mean_squared_error improved from 0.03402 to 0.03402, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 172/250\n",
      "Epoch 00172: mean_squared_error improved from 0.03402 to 0.03401, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 173/250\n",
      "Epoch 00173: mean_squared_error improved from 0.03401 to 0.03400, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 174/250\n",
      "Epoch 00174: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 175/250\n",
      "Epoch 00175: mean_squared_error improved from 0.03400 to 0.03400, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 176/250\n",
      "Epoch 00176: mean_squared_error improved from 0.03400 to 0.03399, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 177/250\n",
      "Epoch 00177: mean_squared_error improved from 0.03399 to 0.03398, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 178/250\n",
      "Epoch 00178: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 179/250\n",
      "Epoch 00179: mean_squared_error improved from 0.03398 to 0.03397, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 180/250\n",
      "Epoch 00180: mean_squared_error improved from 0.03397 to 0.03396, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 181/250\n",
      "Epoch 00181: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 182/250\n",
      "Epoch 00182: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 183/250\n",
      "Epoch 00183: mean_squared_error improved from 0.03396 to 0.03395, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 184/250\n",
      "Epoch 00184: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 185/250\n",
      "Epoch 00185: mean_squared_error improved from 0.03395 to 0.03395, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 186/250\n",
      "Epoch 00186: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 187/250\n",
      "Epoch 00187: mean_squared_error improved from 0.03395 to 0.03394, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 188/250\n",
      "Epoch 00188: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 189/250\n",
      "Epoch 00189: mean_squared_error improved from 0.03394 to 0.03393, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 190/250\n",
      "Epoch 00190: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 191/250\n",
      "Epoch 00191: mean_squared_error improved from 0.03393 to 0.03391, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 192/250\n",
      "Epoch 00192: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 193/250\n",
      "Epoch 00193: mean_squared_error improved from 0.03391 to 0.03391, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 194/250\n",
      "Epoch 00194: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 195/250\n",
      "Epoch 00195: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 196/250\n",
      "Epoch 00196: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 197/250\n",
      "Epoch 00197: mean_squared_error improved from 0.03391 to 0.03389, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
      "Epoch 198/250\n",
      "Epoch 00198: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 199/250\n",
      "Epoch 00199: mean_squared_error improved from 0.03389 to 0.03389, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 200/250\n",
      "Epoch 00200: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 201/250\n",
      "Epoch 00201: mean_squared_error improved from 0.03389 to 0.03388, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/250\n",
      "Epoch 00202: mean_squared_error improved from 0.03388 to 0.03388, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 203/250\n",
      "Epoch 00203: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 204/250\n",
      "Epoch 00204: mean_squared_error improved from 0.03388 to 0.03388, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 205/250\n",
      "Epoch 00205: mean_squared_error improved from 0.03388 to 0.03387, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 206/250\n",
      "Epoch 00206: mean_squared_error improved from 0.03387 to 0.03386, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 207/250\n",
      "Epoch 00207: mean_squared_error improved from 0.03386 to 0.03386, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 208/250\n",
      "Epoch 00208: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 209/250\n",
      "Epoch 00209: mean_squared_error improved from 0.03386 to 0.03385, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 210/250\n",
      "Epoch 00210: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 211/250\n",
      "Epoch 00211: mean_squared_error improved from 0.03385 to 0.03385, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 212/250\n",
      "Epoch 00212: mean_squared_error improved from 0.03385 to 0.03384, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 213/250\n",
      "Epoch 00213: mean_squared_error improved from 0.03384 to 0.03384, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 214/250\n",
      "Epoch 00214: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 215/250\n",
      "Epoch 00215: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 216/250\n",
      "Epoch 00216: mean_squared_error improved from 0.03384 to 0.03384, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 217/250\n",
      "Epoch 00217: mean_squared_error improved from 0.03384 to 0.03382, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 218/250\n",
      "Epoch 00218: mean_squared_error improved from 0.03382 to 0.03382, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 219/250\n",
      "Epoch 00219: mean_squared_error improved from 0.03382 to 0.03382, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 220/250\n",
      "Epoch 00220: mean_squared_error improved from 0.03382 to 0.03381, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 221/250\n",
      "Epoch 00221: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 222/250\n",
      "Epoch 00222: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 223/250\n",
      "Epoch 00223: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 224/250\n",
      "Epoch 00224: mean_squared_error improved from 0.03381 to 0.03380, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 225/250\n",
      "Epoch 00225: mean_squared_error improved from 0.03380 to 0.03380, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 226/250\n",
      "Epoch 00226: mean_squared_error improved from 0.03380 to 0.03379, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 227/250\n",
      "Epoch 00227: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 228/250\n",
      "Epoch 00228: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 229/250\n",
      "Epoch 00229: mean_squared_error improved from 0.03379 to 0.03379, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 230/250\n",
      "Epoch 00230: mean_squared_error improved from 0.03379 to 0.03378, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 231/250\n",
      "Epoch 00231: mean_squared_error improved from 0.03378 to 0.03378, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 232/250\n",
      "Epoch 00232: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 233/250\n",
      "Epoch 00233: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 234/250\n",
      "Epoch 00234: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 235/250\n",
      "Epoch 00235: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 236/250\n",
      "Epoch 00236: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 237/250\n",
      "Epoch 00237: mean_squared_error improved from 0.03378 to 0.03377, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 238/250\n",
      "Epoch 00238: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 239/250\n",
      "Epoch 00239: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 240/250\n",
      "Epoch 00240: mean_squared_error improved from 0.03377 to 0.03377, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 241/250\n",
      "Epoch 00241: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 242/250\n",
      "Epoch 00242: mean_squared_error improved from 0.03377 to 0.03375, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 243/250\n",
      "Epoch 00243: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 244/250\n",
      "Epoch 00244: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00245: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 246/250\n",
      "Epoch 00246: mean_squared_error improved from 0.03375 to 0.03374, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 247/250\n",
      "Epoch 00247: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 248/250\n",
      "Epoch 00248: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 249/250\n",
      "Epoch 00249: mean_squared_error improved from 0.03374 to 0.03374, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 250/250\n",
      "Epoch 00250: mean_squared_error improved from 0.03374 to 0.03374, saving model to lstm_simple.h5\n",
      " - 1s - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 20, 7)             420       \n",
      "=================================================================\n",
      "Total params: 420\n",
      "Trainable params: 420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training time : 185.00573801994324s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=nb_epoch, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=2, \n",
    "                    callbacks = [checkpoint, early])\n",
    "stop = time.time()\n",
    "t1 = stop-start\n",
    "print(model.summary())\n",
    "print(\"Training time : {}s\".format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_steps = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN\n",
    "\n",
    "Using the same code, we can train the standard RNN. The principle is that every output of every hidden layers, are also feed as entry for the next step\n",
    "\n",
    "<img src=\"SimpleRNN.png\"/>\n",
    "\n",
    "This allows a \"short term memory\". It creates a kind of hysteresis used as memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(SimpleRNN(units=nb_unit, input_shape=inp_shape, return_sequences=True))\n",
    "model2.compile(loss=loss_,\n",
    "              optimizer=optimizer_,\n",
    "              metrics=[metrics_])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"srnn_simple.h5\",\n",
    "    monitor=loss_,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1)\n",
    "early = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 256 samples\n",
      "Epoch 1/250\n",
      "Epoch 00001: mean_squared_error improved from inf to 0.20902, saving model to srnn_simple.h5\n",
      " - 1s - loss: 0.2090 - mean_squared_error: 0.2090 - val_loss: 0.1713 - val_mean_squared_error: 0.1713\n",
      "Epoch 2/250\n",
      "Epoch 00002: mean_squared_error improved from 0.20902 to 0.14580, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.1458 - mean_squared_error: 0.1458 - val_loss: 0.1254 - val_mean_squared_error: 0.1254\n",
      "Epoch 3/250\n",
      "Epoch 00003: mean_squared_error improved from 0.14580 to 0.11278, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.1128 - mean_squared_error: 0.1128 - val_loss: 0.1021 - val_mean_squared_error: 0.1021\n",
      "Epoch 4/250\n",
      "Epoch 00004: mean_squared_error improved from 0.11278 to 0.09488, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0949 - mean_squared_error: 0.0949 - val_loss: 0.0883 - val_mean_squared_error: 0.0883\n",
      "Epoch 5/250\n",
      "Epoch 00005: mean_squared_error improved from 0.09488 to 0.08371, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0837 - mean_squared_error: 0.0837 - val_loss: 0.0792 - val_mean_squared_error: 0.0792\n",
      "Epoch 6/250\n",
      "Epoch 00006: mean_squared_error improved from 0.08371 to 0.07606, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0728 - val_mean_squared_error: 0.0728\n",
      "Epoch 7/250\n",
      "Epoch 00007: mean_squared_error improved from 0.07606 to 0.07054, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.0680 - val_mean_squared_error: 0.0680\n",
      "Epoch 8/250\n",
      "Epoch 00008: mean_squared_error improved from 0.07054 to 0.06643, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0664 - mean_squared_error: 0.0664 - val_loss: 0.0644 - val_mean_squared_error: 0.0644\n",
      "Epoch 9/250\n",
      "Epoch 00009: mean_squared_error improved from 0.06643 to 0.06331, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0633 - mean_squared_error: 0.0633 - val_loss: 0.0617 - val_mean_squared_error: 0.0617\n",
      "Epoch 10/250\n",
      "Epoch 00010: mean_squared_error improved from 0.06331 to 0.06090, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0609 - mean_squared_error: 0.0609 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 11/250\n",
      "Epoch 00011: mean_squared_error improved from 0.06090 to 0.05901, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0590 - mean_squared_error: 0.0590 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 12/250\n",
      "Epoch 00012: mean_squared_error improved from 0.05901 to 0.05753, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0575 - mean_squared_error: 0.0575 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 13/250\n",
      "Epoch 00013: mean_squared_error improved from 0.05753 to 0.05636, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0564 - mean_squared_error: 0.0564 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 14/250\n",
      "Epoch 00014: mean_squared_error improved from 0.05636 to 0.05541, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0554 - mean_squared_error: 0.0554 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 15/250\n",
      "Epoch 00015: mean_squared_error improved from 0.05541 to 0.05462, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0546 - mean_squared_error: 0.0546 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 16/250\n",
      "Epoch 00016: mean_squared_error improved from 0.05462 to 0.05396, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 17/250\n",
      "Epoch 00017: mean_squared_error improved from 0.05396 to 0.05339, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0534 - mean_squared_error: 0.0534 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 18/250\n",
      "Epoch 00018: mean_squared_error improved from 0.05339 to 0.05289, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0529 - mean_squared_error: 0.0529 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 19/250\n",
      "Epoch 00019: mean_squared_error improved from 0.05289 to 0.05244, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0524 - mean_squared_error: 0.0524 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 20/250\n",
      "Epoch 00020: mean_squared_error improved from 0.05244 to 0.05204, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0520 - mean_squared_error: 0.0520 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 21/250\n",
      "Epoch 00021: mean_squared_error improved from 0.05204 to 0.05167, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0517 - mean_squared_error: 0.0517 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 22/250\n",
      "Epoch 00022: mean_squared_error improved from 0.05167 to 0.05133, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0513 - mean_squared_error: 0.0513 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 23/250\n",
      "Epoch 00023: mean_squared_error improved from 0.05133 to 0.05102, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 24/250\n",
      "Epoch 00024: mean_squared_error improved from 0.05102 to 0.05073, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 25/250\n",
      "Epoch 00025: mean_squared_error improved from 0.05073 to 0.05046, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0505 - mean_squared_error: 0.0505 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 26/250\n",
      "Epoch 00026: mean_squared_error improved from 0.05046 to 0.05020, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 27/250\n",
      "Epoch 00027: mean_squared_error improved from 0.05020 to 0.04996, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0500 - mean_squared_error: 0.0500 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 28/250\n",
      "Epoch 00028: mean_squared_error improved from 0.04996 to 0.04974, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 29/250\n",
      "Epoch 00029: mean_squared_error improved from 0.04974 to 0.04952, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0495 - mean_squared_error: 0.0495 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 30/250\n",
      "Epoch 00030: mean_squared_error improved from 0.04952 to 0.04932, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0493 - mean_squared_error: 0.0493 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 31/250\n",
      "Epoch 00031: mean_squared_error improved from 0.04932 to 0.04912, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 32/250\n",
      "Epoch 00032: mean_squared_error improved from 0.04912 to 0.04894, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 33/250\n",
      "Epoch 00033: mean_squared_error improved from 0.04894 to 0.04876, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0488 - mean_squared_error: 0.0488 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 34/250\n",
      "Epoch 00034: mean_squared_error improved from 0.04876 to 0.04859, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 35/250\n",
      "Epoch 00035: mean_squared_error improved from 0.04859 to 0.04842, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 36/250\n",
      "Epoch 00036: mean_squared_error improved from 0.04842 to 0.04827, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0483 - mean_squared_error: 0.0483 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 37/250\n",
      "Epoch 00037: mean_squared_error improved from 0.04827 to 0.04812, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 38/250\n",
      "Epoch 00038: mean_squared_error improved from 0.04812 to 0.04798, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 39/250\n",
      "Epoch 00039: mean_squared_error improved from 0.04798 to 0.04784, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/250\n",
      "Epoch 00040: mean_squared_error improved from 0.04784 to 0.04771, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0477 - mean_squared_error: 0.0477 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 41/250\n",
      "Epoch 00041: mean_squared_error improved from 0.04771 to 0.04758, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0476 - mean_squared_error: 0.0476 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 42/250\n",
      "Epoch 00042: mean_squared_error improved from 0.04758 to 0.04746, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 43/250\n",
      "Epoch 00043: mean_squared_error improved from 0.04746 to 0.04734, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 44/250\n",
      "Epoch 00044: mean_squared_error improved from 0.04734 to 0.04723, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 45/250\n",
      "Epoch 00045: mean_squared_error improved from 0.04723 to 0.04712, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 46/250\n",
      "Epoch 00046: mean_squared_error improved from 0.04712 to 0.04703, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0470 - mean_squared_error: 0.0470 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 47/250\n",
      "Epoch 00047: mean_squared_error improved from 0.04703 to 0.04692, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 48/250\n",
      "Epoch 00048: mean_squared_error improved from 0.04692 to 0.04683, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 49/250\n",
      "Epoch 00049: mean_squared_error improved from 0.04683 to 0.04674, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 50/250\n",
      "Epoch 00050: mean_squared_error improved from 0.04674 to 0.04666, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 51/250\n",
      "Epoch 00051: mean_squared_error improved from 0.04666 to 0.04657, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0466 - mean_squared_error: 0.0466 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 52/250\n",
      "Epoch 00052: mean_squared_error improved from 0.04657 to 0.04650, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 53/250\n",
      "Epoch 00053: mean_squared_error improved from 0.04650 to 0.04642, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 54/250\n",
      "Epoch 00054: mean_squared_error improved from 0.04642 to 0.04636, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 55/250\n",
      "Epoch 00055: mean_squared_error improved from 0.04636 to 0.04629, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 56/250\n",
      "Epoch 00056: mean_squared_error improved from 0.04629 to 0.04624, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 57/250\n",
      "Epoch 00057: mean_squared_error improved from 0.04624 to 0.04617, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 58/250\n",
      "Epoch 00058: mean_squared_error improved from 0.04617 to 0.04612, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 59/250\n",
      "Epoch 00059: mean_squared_error improved from 0.04612 to 0.04607, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 60/250\n",
      "Epoch 00060: mean_squared_error improved from 0.04607 to 0.04603, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 61/250\n",
      "Epoch 00061: mean_squared_error improved from 0.04603 to 0.04598, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 62/250\n",
      "Epoch 00062: mean_squared_error improved from 0.04598 to 0.04593, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 63/250\n",
      "Epoch 00063: mean_squared_error improved from 0.04593 to 0.04589, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 64/250\n",
      "Epoch 00064: mean_squared_error improved from 0.04589 to 0.04585, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 65/250\n",
      "Epoch 00065: mean_squared_error improved from 0.04585 to 0.04582, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 66/250\n",
      "Epoch 00066: mean_squared_error improved from 0.04582 to 0.04579, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 67/250\n",
      "Epoch 00067: mean_squared_error improved from 0.04579 to 0.04576, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 68/250\n",
      "Epoch 00068: mean_squared_error improved from 0.04576 to 0.04572, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 69/250\n",
      "Epoch 00069: mean_squared_error improved from 0.04572 to 0.04570, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 70/250\n",
      "Epoch 00070: mean_squared_error improved from 0.04570 to 0.04567, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 71/250\n",
      "Epoch 00071: mean_squared_error improved from 0.04567 to 0.04566, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 72/250\n",
      "Epoch 00072: mean_squared_error improved from 0.04566 to 0.04562, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 73/250\n",
      "Epoch 00073: mean_squared_error improved from 0.04562 to 0.04560, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 74/250\n",
      "Epoch 00074: mean_squared_error improved from 0.04560 to 0.04558, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 75/250\n",
      "Epoch 00075: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 76/250\n",
      "Epoch 00076: mean_squared_error improved from 0.04558 to 0.04556, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 77/250\n",
      "Epoch 00077: mean_squared_error improved from 0.04556 to 0.04552, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 78/250\n",
      "Epoch 00078: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 79/250\n",
      "Epoch 00079: mean_squared_error improved from 0.04552 to 0.04549, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/250\n",
      "Epoch 00080: mean_squared_error improved from 0.04549 to 0.04548, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 81/250\n",
      "Epoch 00081: mean_squared_error improved from 0.04548 to 0.04548, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 82/250\n",
      "Epoch 00082: mean_squared_error improved from 0.04548 to 0.04546, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 83/250\n",
      "Epoch 00083: mean_squared_error improved from 0.04546 to 0.04544, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 84/250\n",
      "Epoch 00084: mean_squared_error improved from 0.04544 to 0.04543, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 85/250\n",
      "Epoch 00085: mean_squared_error improved from 0.04543 to 0.04542, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 86/250\n",
      "Epoch 00086: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 87/250\n",
      "Epoch 00087: mean_squared_error improved from 0.04542 to 0.04540, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 88/250\n",
      "Epoch 00088: mean_squared_error improved from 0.04540 to 0.04539, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 89/250\n",
      "Epoch 00089: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 90/250\n",
      "Epoch 00090: mean_squared_error improved from 0.04539 to 0.04537, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 91/250\n",
      "Epoch 00091: mean_squared_error improved from 0.04537 to 0.04537, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 92/250\n",
      "Epoch 00092: mean_squared_error improved from 0.04537 to 0.04536, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 93/250\n",
      "Epoch 00093: mean_squared_error improved from 0.04536 to 0.04534, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 94/250\n",
      "Epoch 00094: mean_squared_error improved from 0.04534 to 0.04533, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 95/250\n",
      "Epoch 00095: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 96/250\n",
      "Epoch 00096: mean_squared_error improved from 0.04533 to 0.04532, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 97/250\n",
      "Epoch 00097: mean_squared_error improved from 0.04532 to 0.04532, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 98/250\n",
      "Epoch 00098: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 99/250\n",
      "Epoch 00099: mean_squared_error improved from 0.04532 to 0.04531, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 100/250\n",
      "Epoch 00100: mean_squared_error improved from 0.04531 to 0.04530, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 101/250\n",
      "Epoch 00101: mean_squared_error improved from 0.04530 to 0.04529, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 102/250\n",
      "Epoch 00102: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 103/250\n",
      "Epoch 00103: mean_squared_error improved from 0.04529 to 0.04529, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 104/250\n",
      "Epoch 00104: mean_squared_error improved from 0.04529 to 0.04527, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 105/250\n",
      "Epoch 00105: mean_squared_error improved from 0.04527 to 0.04527, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 106/250\n",
      "Epoch 00106: mean_squared_error improved from 0.04527 to 0.04527, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 107/250\n",
      "Epoch 00107: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 108/250\n",
      "Epoch 00108: mean_squared_error improved from 0.04527 to 0.04525, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 109/250\n",
      "Epoch 00109: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 110/250\n",
      "Epoch 00110: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 111/250\n",
      "Epoch 00111: mean_squared_error improved from 0.04525 to 0.04524, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 112/250\n",
      "Epoch 00112: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 113/250\n",
      "Epoch 00113: mean_squared_error improved from 0.04524 to 0.04523, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 114/250\n",
      "Epoch 00114: mean_squared_error improved from 0.04523 to 0.04523, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 115/250\n",
      "Epoch 00115: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 116/250\n",
      "Epoch 00116: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 117/250\n",
      "Epoch 00117: mean_squared_error improved from 0.04523 to 0.04521, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 118/250\n",
      "Epoch 00118: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 119/250\n",
      "Epoch 00119: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 120/250\n",
      "Epoch 00120: mean_squared_error improved from 0.04521 to 0.04521, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 121/250\n",
      "Epoch 00121: mean_squared_error improved from 0.04521 to 0.04520, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/250\n",
      "Epoch 00122: mean_squared_error improved from 0.04520 to 0.04520, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 123/250\n",
      "Epoch 00123: mean_squared_error improved from 0.04520 to 0.04520, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 124/250\n",
      "Epoch 00124: mean_squared_error improved from 0.04520 to 0.04519, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 125/250\n",
      "Epoch 00125: mean_squared_error improved from 0.04519 to 0.04519, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 126/250\n",
      "Epoch 00126: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 127/250\n",
      "Epoch 00127: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 128/250\n",
      "Epoch 00128: mean_squared_error improved from 0.04519 to 0.04518, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 129/250\n",
      "Epoch 00129: mean_squared_error improved from 0.04518 to 0.04518, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 130/250\n",
      "Epoch 00130: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 131/250\n",
      "Epoch 00131: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 132/250\n",
      "Epoch 00132: mean_squared_error improved from 0.04518 to 0.04517, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 133/250\n",
      "Epoch 00133: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 134/250\n",
      "Epoch 00134: mean_squared_error improved from 0.04517 to 0.04516, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 135/250\n",
      "Epoch 00135: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 136/250\n",
      "Epoch 00136: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 137/250\n",
      "Epoch 00137: mean_squared_error improved from 0.04516 to 0.04516, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 138/250\n",
      "Epoch 00138: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 139/250\n",
      "Epoch 00139: mean_squared_error improved from 0.04516 to 0.04516, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 140/250\n",
      "Epoch 00140: mean_squared_error improved from 0.04516 to 0.04516, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 141/250\n",
      "Epoch 00141: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 142/250\n",
      "Epoch 00142: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 143/250\n",
      "Epoch 00143: mean_squared_error improved from 0.04516 to 0.04515, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 144/250\n",
      "Epoch 00144: mean_squared_error improved from 0.04515 to 0.04514, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 145/250\n",
      "Epoch 00145: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 146/250\n",
      "Epoch 00146: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 147/250\n",
      "Epoch 00147: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 148/250\n",
      "Epoch 00148: mean_squared_error improved from 0.04514 to 0.04513, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 149/250\n",
      "Epoch 00149: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 150/250\n",
      "Epoch 00150: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 151/250\n",
      "Epoch 00151: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 152/250\n",
      "Epoch 00152: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 153/250\n",
      "Epoch 00153: mean_squared_error improved from 0.04513 to 0.04513, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 154/250\n",
      "Epoch 00154: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 155/250\n",
      "Epoch 00155: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 156/250\n",
      "Epoch 00156: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 157/250\n",
      "Epoch 00157: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 158/250\n",
      "Epoch 00158: mean_squared_error improved from 0.04513 to 0.04513, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 159/250\n",
      "Epoch 00159: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 160/250\n",
      "Epoch 00160: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 161/250\n",
      "Epoch 00161: mean_squared_error improved from 0.04513 to 0.04512, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 162/250\n",
      "Epoch 00162: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 163/250\n",
      "Epoch 00163: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 164/250\n",
      "Epoch 00164: mean_squared_error improved from 0.04512 to 0.04512, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 165/250\n",
      "Epoch 00165: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 166/250\n",
      "Epoch 00166: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/250\n",
      "Epoch 00167: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 168/250\n",
      "Epoch 00168: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 169/250\n",
      "Epoch 00169: mean_squared_error improved from 0.04512 to 0.04511, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 170/250\n",
      "Epoch 00170: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 171/250\n",
      "Epoch 00171: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 172/250\n",
      "Epoch 00172: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 173/250\n",
      "Epoch 00173: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 174/250\n",
      "Epoch 00174: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 175/250\n",
      "Epoch 00175: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 176/250\n",
      "Epoch 00176: mean_squared_error improved from 0.04511 to 0.04510, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 177/250\n",
      "Epoch 00177: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 178/250\n",
      "Epoch 00178: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 179/250\n",
      "Epoch 00179: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 180/250\n",
      "Epoch 00180: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 181/250\n",
      "Epoch 00181: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 182/250\n",
      "Epoch 00182: mean_squared_error improved from 0.04510 to 0.04510, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 183/250\n",
      "Epoch 00183: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 184/250\n",
      "Epoch 00184: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 185/250\n",
      "Epoch 00185: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 186/250\n",
      "Epoch 00186: mean_squared_error improved from 0.04510 to 0.04510, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 187/250\n",
      "Epoch 00187: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 188/250\n",
      "Epoch 00188: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 189/250\n",
      "Epoch 00189: mean_squared_error improved from 0.04510 to 0.04510, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 190/250\n",
      "Epoch 00190: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 191/250\n",
      "Epoch 00191: mean_squared_error improved from 0.04510 to 0.04509, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 192/250\n",
      "Epoch 00192: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 193/250\n",
      "Epoch 00193: mean_squared_error improved from 0.04509 to 0.04509, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 194/250\n",
      "Epoch 00194: mean_squared_error improved from 0.04509 to 0.04509, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 195/250\n",
      "Epoch 00195: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 196/250\n",
      "Epoch 00196: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 197/250\n",
      "Epoch 00197: mean_squared_error improved from 0.04509 to 0.04508, saving model to srnn_simple.h5\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 198/250\n",
      "Epoch 00198: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 199/250\n",
      "Epoch 00199: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 200/250\n",
      "Epoch 00200: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 201/250\n",
      "Epoch 00201: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 202/250\n",
      "Epoch 00202: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 203/250\n",
      "Epoch 00203: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 204/250\n",
      "Epoch 00204: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 205/250\n",
      "Epoch 00205: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 206/250\n",
      "Epoch 00206: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 207/250\n",
      "Epoch 00207: mean_squared_error did not improve\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 00207: early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 20, 7)             105       \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None Training time : 53.972092151641846s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history2 = model2.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=nb_epoch, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=2, \n",
    "                    callbacks = [checkpoint, early])\n",
    "stop = time.time()\n",
    "t2 = stop-start\n",
    "print(model2.summary(), end=\" \")\n",
    "print(\"Training time : {}s\".format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRNN_steps = 198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n",
    "Finally, we can train a <b>GRU</b> (<b>G</b>ated <b>R</b>ecurrent <b>U</b>nits). It's a simplification of LSTMs. They also have a memory mechanism but with less parameters. As a result they are faster to train. You can find differences on <a href=\"https://datascience.stackexchange.com/questions/14581/what-is-difference-between-gru-and-lstm\" target=\"_blank\">this topic</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(GRU(units=nb_unit, input_shape=inp_shape, return_sequences=True))\n",
    "model3.compile(loss=loss_,\n",
    "              optimizer=optimizer_,\n",
    "              metrics=[metrics_])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"gru_simple.h5\",\n",
    "    monitor=loss_,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1)\n",
    "early = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 256 samples\n",
      "Epoch 1/250\n",
      "Epoch 00001: mean_squared_error improved from inf to 0.08024, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0802 - mean_squared_error: 0.0802 - val_loss: 0.0734 - val_mean_squared_error: 0.0734\n",
      "Epoch 2/250\n",
      "Epoch 00002: mean_squared_error improved from 0.08024 to 0.07086, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0709 - mean_squared_error: 0.0709 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
      "Epoch 3/250\n",
      "Epoch 00003: mean_squared_error improved from 0.07086 to 0.06643, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0664 - mean_squared_error: 0.0664 - val_loss: 0.0642 - val_mean_squared_error: 0.0642\n",
      "Epoch 4/250\n",
      "Epoch 00004: mean_squared_error improved from 0.06643 to 0.06312, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0631 - mean_squared_error: 0.0631 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 5/250\n",
      "Epoch 00005: mean_squared_error improved from 0.06312 to 0.06025, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0602 - mean_squared_error: 0.0602 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 6/250\n",
      "Epoch 00006: mean_squared_error improved from 0.06025 to 0.05758, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0576 - mean_squared_error: 0.0576 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 7/250\n",
      "Epoch 00007: mean_squared_error improved from 0.05758 to 0.05507, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0551 - mean_squared_error: 0.0551 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 8/250\n",
      "Epoch 00008: mean_squared_error improved from 0.05507 to 0.05275, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 9/250\n",
      "Epoch 00009: mean_squared_error improved from 0.05275 to 0.05064, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0506 - mean_squared_error: 0.0506 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 10/250\n",
      "Epoch 00010: mean_squared_error improved from 0.05064 to 0.04873, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 11/250\n",
      "Epoch 00011: mean_squared_error improved from 0.04873 to 0.04706, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 12/250\n",
      "Epoch 00012: mean_squared_error improved from 0.04706 to 0.04577, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 13/250\n",
      "Epoch 00013: mean_squared_error improved from 0.04577 to 0.04477, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 14/250\n",
      "Epoch 00014: mean_squared_error improved from 0.04477 to 0.04395, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 15/250\n",
      "Epoch 00015: mean_squared_error improved from 0.04395 to 0.04325, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 16/250\n",
      "Epoch 00016: mean_squared_error improved from 0.04325 to 0.04260, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 17/250\n",
      "Epoch 00017: mean_squared_error improved from 0.04260 to 0.04198, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 18/250\n",
      "Epoch 00018: mean_squared_error improved from 0.04198 to 0.04140, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 19/250\n",
      "Epoch 00019: mean_squared_error improved from 0.04140 to 0.04084, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 20/250\n",
      "Epoch 00020: mean_squared_error improved from 0.04084 to 0.04031, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 21/250\n",
      "Epoch 00021: mean_squared_error improved from 0.04031 to 0.03981, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 22/250\n",
      "Epoch 00022: mean_squared_error improved from 0.03981 to 0.03937, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 23/250\n",
      "Epoch 00023: mean_squared_error improved from 0.03937 to 0.03899, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 24/250\n",
      "Epoch 00024: mean_squared_error improved from 0.03899 to 0.03866, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 25/250\n",
      "Epoch 00025: mean_squared_error improved from 0.03866 to 0.03839, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 26/250\n",
      "Epoch 00026: mean_squared_error improved from 0.03839 to 0.03816, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 27/250\n",
      "Epoch 00027: mean_squared_error improved from 0.03816 to 0.03797, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 28/250\n",
      "Epoch 00028: mean_squared_error improved from 0.03797 to 0.03779, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 29/250\n",
      "Epoch 00029: mean_squared_error improved from 0.03779 to 0.03764, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 30/250\n",
      "Epoch 00030: mean_squared_error improved from 0.03764 to 0.03752, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 31/250\n",
      "Epoch 00031: mean_squared_error improved from 0.03752 to 0.03739, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 32/250\n",
      "Epoch 00032: mean_squared_error improved from 0.03739 to 0.03728, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 33/250\n",
      "Epoch 00033: mean_squared_error improved from 0.03728 to 0.03719, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 34/250\n",
      "Epoch 00034: mean_squared_error improved from 0.03719 to 0.03709, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 35/250\n",
      "Epoch 00035: mean_squared_error improved from 0.03709 to 0.03701, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 36/250\n",
      "Epoch 00036: mean_squared_error improved from 0.03701 to 0.03694, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 37/250\n",
      "Epoch 00037: mean_squared_error improved from 0.03694 to 0.03686, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 38/250\n",
      "Epoch 00038: mean_squared_error improved from 0.03686 to 0.03679, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 39/250\n",
      "Epoch 00039: mean_squared_error improved from 0.03679 to 0.03670, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/250\n",
      "Epoch 00040: mean_squared_error improved from 0.03670 to 0.03664, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 41/250\n",
      "Epoch 00041: mean_squared_error improved from 0.03664 to 0.03658, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 42/250\n",
      "Epoch 00042: mean_squared_error improved from 0.03658 to 0.03655, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 43/250\n",
      "Epoch 00043: mean_squared_error improved from 0.03655 to 0.03649, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 44/250\n",
      "Epoch 00044: mean_squared_error improved from 0.03649 to 0.03643, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 45/250\n",
      "Epoch 00045: mean_squared_error improved from 0.03643 to 0.03640, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 46/250\n",
      "Epoch 00046: mean_squared_error improved from 0.03640 to 0.03635, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 47/250\n",
      "Epoch 00047: mean_squared_error improved from 0.03635 to 0.03631, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 48/250\n",
      "Epoch 00048: mean_squared_error improved from 0.03631 to 0.03627, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 49/250\n",
      "Epoch 00049: mean_squared_error improved from 0.03627 to 0.03624, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 50/250\n",
      "Epoch 00050: mean_squared_error improved from 0.03624 to 0.03621, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 51/250\n",
      "Epoch 00051: mean_squared_error improved from 0.03621 to 0.03618, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 52/250\n",
      "Epoch 00052: mean_squared_error improved from 0.03618 to 0.03612, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 53/250\n",
      "Epoch 00053: mean_squared_error improved from 0.03612 to 0.03612, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 54/250\n",
      "Epoch 00054: mean_squared_error improved from 0.03612 to 0.03608, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 55/250\n",
      "Epoch 00055: mean_squared_error improved from 0.03608 to 0.03607, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 56/250\n",
      "Epoch 00056: mean_squared_error improved from 0.03607 to 0.03604, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 57/250\n",
      "Epoch 00057: mean_squared_error improved from 0.03604 to 0.03600, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 58/250\n",
      "Epoch 00058: mean_squared_error improved from 0.03600 to 0.03598, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 59/250\n",
      "Epoch 00059: mean_squared_error improved from 0.03598 to 0.03597, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 60/250\n",
      "Epoch 00060: mean_squared_error improved from 0.03597 to 0.03594, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 61/250\n",
      "Epoch 00061: mean_squared_error improved from 0.03594 to 0.03592, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 62/250\n",
      "Epoch 00062: mean_squared_error improved from 0.03592 to 0.03591, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 63/250\n",
      "Epoch 00063: mean_squared_error improved from 0.03591 to 0.03587, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 64/250\n",
      "Epoch 00064: mean_squared_error improved from 0.03587 to 0.03586, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 65/250\n",
      "Epoch 00065: mean_squared_error improved from 0.03586 to 0.03582, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 66/250\n",
      "Epoch 00066: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 67/250\n",
      "Epoch 00067: mean_squared_error improved from 0.03582 to 0.03579, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 68/250\n",
      "Epoch 00068: mean_squared_error improved from 0.03579 to 0.03577, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 69/250\n",
      "Epoch 00069: mean_squared_error improved from 0.03577 to 0.03574, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 70/250\n",
      "Epoch 00070: mean_squared_error improved from 0.03574 to 0.03573, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 71/250\n",
      "Epoch 00071: mean_squared_error improved from 0.03573 to 0.03572, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 72/250\n",
      "Epoch 00072: mean_squared_error improved from 0.03572 to 0.03569, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 73/250\n",
      "Epoch 00073: mean_squared_error improved from 0.03569 to 0.03567, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 74/250\n",
      "Epoch 00074: mean_squared_error improved from 0.03567 to 0.03565, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 75/250\n",
      "Epoch 00075: mean_squared_error improved from 0.03565 to 0.03563, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 76/250\n",
      "Epoch 00076: mean_squared_error improved from 0.03563 to 0.03561, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 77/250\n",
      "Epoch 00077: mean_squared_error improved from 0.03561 to 0.03559, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 78/250\n",
      "Epoch 00078: mean_squared_error improved from 0.03559 to 0.03559, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 79/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00079: mean_squared_error improved from 0.03559 to 0.03553, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 80/250\n",
      "Epoch 00080: mean_squared_error improved from 0.03553 to 0.03553, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 81/250\n",
      "Epoch 00081: mean_squared_error improved from 0.03553 to 0.03551, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 82/250\n",
      "Epoch 00082: mean_squared_error improved from 0.03551 to 0.03548, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 83/250\n",
      "Epoch 00083: mean_squared_error improved from 0.03548 to 0.03548, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 84/250\n",
      "Epoch 00084: mean_squared_error improved from 0.03548 to 0.03544, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 85/250\n",
      "Epoch 00085: mean_squared_error improved from 0.03544 to 0.03544, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 86/250\n",
      "Epoch 00086: mean_squared_error improved from 0.03544 to 0.03542, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 87/250\n",
      "Epoch 00087: mean_squared_error improved from 0.03542 to 0.03541, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 88/250\n",
      "Epoch 00088: mean_squared_error improved from 0.03541 to 0.03538, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 89/250\n",
      "Epoch 00089: mean_squared_error improved from 0.03538 to 0.03537, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 90/250\n",
      "Epoch 00090: mean_squared_error improved from 0.03537 to 0.03535, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 91/250\n",
      "Epoch 00091: mean_squared_error improved from 0.03535 to 0.03534, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 92/250\n",
      "Epoch 00092: mean_squared_error improved from 0.03534 to 0.03531, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 93/250\n",
      "Epoch 00093: mean_squared_error improved from 0.03531 to 0.03531, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 94/250\n",
      "Epoch 00094: mean_squared_error improved from 0.03531 to 0.03527, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 95/250\n",
      "Epoch 00095: mean_squared_error improved from 0.03527 to 0.03527, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 96/250\n",
      "Epoch 00096: mean_squared_error improved from 0.03527 to 0.03524, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 97/250\n",
      "Epoch 00097: mean_squared_error improved from 0.03524 to 0.03522, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 98/250\n",
      "Epoch 00098: mean_squared_error improved from 0.03522 to 0.03521, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 99/250\n",
      "Epoch 00099: mean_squared_error improved from 0.03521 to 0.03520, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 100/250\n",
      "Epoch 00100: mean_squared_error improved from 0.03520 to 0.03518, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 101/250\n",
      "Epoch 00101: mean_squared_error improved from 0.03518 to 0.03518, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 102/250\n",
      "Epoch 00102: mean_squared_error improved from 0.03518 to 0.03515, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 103/250\n",
      "Epoch 00103: mean_squared_error improved from 0.03515 to 0.03514, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 104/250\n",
      "Epoch 00104: mean_squared_error improved from 0.03514 to 0.03513, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 105/250\n",
      "Epoch 00105: mean_squared_error improved from 0.03513 to 0.03511, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 106/250\n",
      "Epoch 00106: mean_squared_error improved from 0.03511 to 0.03510, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 107/250\n",
      "Epoch 00107: mean_squared_error improved from 0.03510 to 0.03509, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 108/250\n",
      "Epoch 00108: mean_squared_error improved from 0.03509 to 0.03509, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 109/250\n",
      "Epoch 00109: mean_squared_error improved from 0.03509 to 0.03507, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 110/250\n",
      "Epoch 00110: mean_squared_error improved from 0.03507 to 0.03504, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 111/250\n",
      "Epoch 00111: mean_squared_error improved from 0.03504 to 0.03504, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 112/250\n",
      "Epoch 00112: mean_squared_error improved from 0.03504 to 0.03502, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 113/250\n",
      "Epoch 00113: mean_squared_error improved from 0.03502 to 0.03502, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 114/250\n",
      "Epoch 00114: mean_squared_error improved from 0.03502 to 0.03501, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 115/250\n",
      "Epoch 00115: mean_squared_error improved from 0.03501 to 0.03498, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 116/250\n",
      "Epoch 00116: mean_squared_error improved from 0.03498 to 0.03498, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 117/250\n",
      "Epoch 00117: mean_squared_error improved from 0.03498 to 0.03496, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/250\n",
      "Epoch 00118: mean_squared_error improved from 0.03496 to 0.03496, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 119/250\n",
      "Epoch 00119: mean_squared_error improved from 0.03496 to 0.03496, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 120/250\n",
      "Epoch 00120: mean_squared_error improved from 0.03496 to 0.03494, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 121/250\n",
      "Epoch 00121: mean_squared_error improved from 0.03494 to 0.03493, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 122/250\n",
      "Epoch 00122: mean_squared_error improved from 0.03493 to 0.03492, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 123/250\n",
      "Epoch 00123: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 124/250\n",
      "Epoch 00124: mean_squared_error improved from 0.03492 to 0.03489, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 125/250\n",
      "Epoch 00125: mean_squared_error improved from 0.03489 to 0.03488, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 126/250\n",
      "Epoch 00126: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 127/250\n",
      "Epoch 00127: mean_squared_error improved from 0.03488 to 0.03487, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 128/250\n",
      "Epoch 00128: mean_squared_error improved from 0.03487 to 0.03486, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 129/250\n",
      "Epoch 00129: mean_squared_error improved from 0.03486 to 0.03485, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 130/250\n",
      "Epoch 00130: mean_squared_error improved from 0.03485 to 0.03483, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 131/250\n",
      "Epoch 00131: mean_squared_error improved from 0.03483 to 0.03482, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 132/250\n",
      "Epoch 00132: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 133/250\n",
      "Epoch 00133: mean_squared_error improved from 0.03482 to 0.03481, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 134/250\n",
      "Epoch 00134: mean_squared_error improved from 0.03481 to 0.03480, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 135/250\n",
      "Epoch 00135: mean_squared_error improved from 0.03480 to 0.03480, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 136/250\n",
      "Epoch 00136: mean_squared_error improved from 0.03480 to 0.03478, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 137/250\n",
      "Epoch 00137: mean_squared_error improved from 0.03478 to 0.03476, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 138/250\n",
      "Epoch 00138: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 139/250\n",
      "Epoch 00139: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 140/250\n",
      "Epoch 00140: mean_squared_error improved from 0.03476 to 0.03474, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 141/250\n",
      "Epoch 00141: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 142/250\n",
      "Epoch 00142: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 143/250\n",
      "Epoch 00143: mean_squared_error improved from 0.03474 to 0.03472, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 144/250\n",
      "Epoch 00144: mean_squared_error improved from 0.03472 to 0.03471, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 145/250\n",
      "Epoch 00145: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 146/250\n",
      "Epoch 00146: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 147/250\n",
      "Epoch 00147: mean_squared_error improved from 0.03471 to 0.03470, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 148/250\n",
      "Epoch 00148: mean_squared_error improved from 0.03470 to 0.03468, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 149/250\n",
      "Epoch 00149: mean_squared_error improved from 0.03468 to 0.03468, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 150/250\n",
      "Epoch 00150: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 151/250\n",
      "Epoch 00151: mean_squared_error improved from 0.03468 to 0.03467, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 152/250\n",
      "Epoch 00152: mean_squared_error improved from 0.03467 to 0.03466, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 153/250\n",
      "Epoch 00153: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 154/250\n",
      "Epoch 00154: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 155/250\n",
      "Epoch 00155: mean_squared_error improved from 0.03466 to 0.03465, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 156/250\n",
      "Epoch 00156: mean_squared_error improved from 0.03465 to 0.03462, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 157/250\n",
      "Epoch 00157: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 158/250\n",
      "Epoch 00158: mean_squared_error improved from 0.03462 to 0.03462, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 159/250\n",
      "Epoch 00159: mean_squared_error improved from 0.03462 to 0.03461, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/250\n",
      "Epoch 00160: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 161/250\n",
      "Epoch 00161: mean_squared_error improved from 0.03461 to 0.03460, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 162/250\n",
      "Epoch 00162: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 163/250\n",
      "Epoch 00163: mean_squared_error improved from 0.03460 to 0.03460, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 164/250\n",
      "Epoch 00164: mean_squared_error improved from 0.03460 to 0.03459, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 165/250\n",
      "Epoch 00165: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 166/250\n",
      "Epoch 00166: mean_squared_error improved from 0.03459 to 0.03458, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 167/250\n",
      "Epoch 00167: mean_squared_error improved from 0.03458 to 0.03457, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 168/250\n",
      "Epoch 00168: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 169/250\n",
      "Epoch 00169: mean_squared_error improved from 0.03457 to 0.03456, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 170/250\n",
      "Epoch 00170: mean_squared_error improved from 0.03456 to 0.03455, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 171/250\n",
      "Epoch 00171: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 172/250\n",
      "Epoch 00172: mean_squared_error improved from 0.03455 to 0.03454, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 173/250\n",
      "Epoch 00173: mean_squared_error improved from 0.03454 to 0.03453, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 174/250\n",
      "Epoch 00174: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 175/250\n",
      "Epoch 00175: mean_squared_error improved from 0.03453 to 0.03453, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 176/250\n",
      "Epoch 00176: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 177/250\n",
      "Epoch 00177: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 178/250\n",
      "Epoch 00178: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 179/250\n",
      "Epoch 00179: mean_squared_error improved from 0.03453 to 0.03452, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 180/250\n",
      "Epoch 00180: mean_squared_error improved from 0.03452 to 0.03451, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 181/250\n",
      "Epoch 00181: mean_squared_error improved from 0.03451 to 0.03450, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 182/250\n",
      "Epoch 00182: mean_squared_error improved from 0.03450 to 0.03450, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 183/250\n",
      "Epoch 00183: mean_squared_error improved from 0.03450 to 0.03449, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 184/250\n",
      "Epoch 00184: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 185/250\n",
      "Epoch 00185: mean_squared_error improved from 0.03449 to 0.03449, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 186/250\n",
      "Epoch 00186: mean_squared_error improved from 0.03449 to 0.03447, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 187/250\n",
      "Epoch 00187: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 188/250\n",
      "Epoch 00188: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 189/250\n",
      "Epoch 00189: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 190/250\n",
      "Epoch 00190: mean_squared_error improved from 0.03447 to 0.03446, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 191/250\n",
      "Epoch 00191: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 192/250\n",
      "Epoch 00192: mean_squared_error improved from 0.03446 to 0.03446, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 193/250\n",
      "Epoch 00193: mean_squared_error improved from 0.03446 to 0.03445, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 194/250\n",
      "Epoch 00194: mean_squared_error improved from 0.03445 to 0.03444, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 195/250\n",
      "Epoch 00195: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 196/250\n",
      "Epoch 00196: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 197/250\n",
      "Epoch 00197: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 198/250\n",
      "Epoch 00198: mean_squared_error improved from 0.03444 to 0.03443, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 199/250\n",
      "Epoch 00199: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 200/250\n",
      "Epoch 00200: mean_squared_error improved from 0.03443 to 0.03443, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 201/250\n",
      "Epoch 00201: mean_squared_error improved from 0.03443 to 0.03443, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 202/250\n",
      "Epoch 00202: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/250\n",
      "Epoch 00203: mean_squared_error improved from 0.03443 to 0.03442, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 204/250\n",
      "Epoch 00204: mean_squared_error improved from 0.03442 to 0.03441, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 205/250\n",
      "Epoch 00205: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 206/250\n",
      "Epoch 00206: mean_squared_error improved from 0.03441 to 0.03441, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 207/250\n",
      "Epoch 00207: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 208/250\n",
      "Epoch 00208: mean_squared_error improved from 0.03441 to 0.03439, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 209/250\n",
      "Epoch 00209: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 210/250\n",
      "Epoch 00210: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 211/250\n",
      "Epoch 00211: mean_squared_error improved from 0.03439 to 0.03438, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 212/250\n",
      "Epoch 00212: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 213/250\n",
      "Epoch 00213: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0341 - val_mean_squared_error: 0.0341\n",
      "Epoch 214/250\n",
      "Epoch 00214: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 215/250\n",
      "Epoch 00215: mean_squared_error improved from 0.03438 to 0.03438, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 216/250\n",
      "Epoch 00216: mean_squared_error improved from 0.03438 to 0.03437, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 217/250\n",
      "Epoch 00217: mean_squared_error improved from 0.03437 to 0.03437, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 218/250\n",
      "Epoch 00218: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 219/250\n",
      "Epoch 00219: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 220/250\n",
      "Epoch 00220: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 221/250\n",
      "Epoch 00221: mean_squared_error improved from 0.03437 to 0.03436, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 222/250\n",
      "Epoch 00222: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 223/250\n",
      "Epoch 00223: mean_squared_error improved from 0.03436 to 0.03436, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 224/250\n",
      "Epoch 00224: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 225/250\n",
      "Epoch 00225: mean_squared_error improved from 0.03436 to 0.03435, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 226/250\n",
      "Epoch 00226: mean_squared_error improved from 0.03435 to 0.03435, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 227/250\n",
      "Epoch 00227: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 228/250\n",
      "Epoch 00228: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 229/250\n",
      "Epoch 00229: mean_squared_error improved from 0.03435 to 0.03435, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
      "Epoch 230/250\n",
      "Epoch 00230: mean_squared_error improved from 0.03435 to 0.03434, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 231/250\n",
      "Epoch 00231: mean_squared_error improved from 0.03434 to 0.03433, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 232/250\n",
      "Epoch 00232: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 233/250\n",
      "Epoch 00233: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 234/250\n",
      "Epoch 00234: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 235/250\n",
      "Epoch 00235: mean_squared_error improved from 0.03433 to 0.03432, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 236/250\n",
      "Epoch 00236: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 237/250\n",
      "Epoch 00237: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 238/250\n",
      "Epoch 00238: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 239/250\n",
      "Epoch 00239: mean_squared_error improved from 0.03432 to 0.03431, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 240/250\n",
      "Epoch 00240: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 241/250\n",
      "Epoch 00241: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 242/250\n",
      "Epoch 00242: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 243/250\n",
      "Epoch 00243: mean_squared_error improved from 0.03431 to 0.03430, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 244/250\n",
      "Epoch 00244: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 245/250\n",
      "Epoch 00245: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 246/250\n",
      "Epoch 00246: mean_squared_error improved from 0.03430 to 0.03428, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 247/250\n",
      "Epoch 00247: mean_squared_error improved from 0.03428 to 0.03427, saving model to gru_simple.h5\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/250\n",
      "Epoch 00248: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 249/250\n",
      "Epoch 00249: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 250/250\n",
      "Epoch 00250: mean_squared_error did not improve\n",
      " - 1s - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 20, 7)             315       \n",
      "=================================================================\n",
      "Total params: 315\n",
      "Trainable params: 315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None Training time : 152.4062421321869s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history3 = model3.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=nb_epoch, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=2, \n",
    "                    callbacks = [checkpoint, early])\n",
    "stop = time.time()\n",
    "t3 = stop-start\n",
    "print(model3.summary(), end=\" \")\n",
    "print(\"Training time : {}s\".format(t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRU_steps = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can first check the time used to train them on the same dataset with the same number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM :       185.01s\n",
      "Simple RNN : 53.97s\n",
      "GRU :        152.41s\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM :       {:.2f}s\".format(t1))\n",
    "print(\"Simple RNN : {:.2f}s\".format(t2))\n",
    "print(\"GRU :        {:.2f}s\".format(t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the simple RNN is the fastest to train because there is nearly no impact of provide the output as input. It's only and addtion to do on Matrices. However, LSTM and GRU are slower to train and as expected, GRU trained faster than LSTM. Ze can also check the error of prediction. For this I used a Mean Absolute Error as loss fonction and Mean Squared Error as metrics . I used this loss fonction to have the norm 1 between prediction and real output. With the Means Squared Error, the training would take longer as the loss would be smaller as all outputs are below 1. For the metric, I used it to ease the visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250, 198, 250]\n"
     ]
    }
   ],
   "source": [
    "print([LSTM_steps, SRNN_steps, GRU_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAHfCAYAAADgETZhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XuclnWd//H3MAMiBwlWQAFJhDUy\nl1KINA0zrMyzlopn8lSslokHFFIJkNVQqWzVLP1lqJmyZKWYtXlWMJvVCA+YJ0JQyQVUkIMzc//+\ncJtdgjwxMPOV5/Px8PHgnuu+r/tzz/318uXFNfdUVSqVSgAAgBavVXMPAAAAvDPiHQAACiHeAQCg\nEOIdAAAKId4BAKAQ4h0AAAoh3v/Ho48+2twj0AJZF6yNdcHaWBesjXVBUxPv/2PFihXNPQItkHXB\n2lgXrI11wdpYFzQ18Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjx\nDgAAhRDvAABQCPEOAACFEO8AAFCIqkqlUmnuIVqCa2a+2twjAADv0u3Ta5t7BHjXrh+3+3t+rDPv\nAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAA\nUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI\n8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEO\nAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAA\nhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ\n7wAAUAjxDgAAhahp7gGS5MEHH8wNN9yQyZMnN35t7ty5Of/881NfX5+6urpsv/32Oe2003L11Vfn\n7rvvzquvvpqFCxemX79+SZIf//jH2W677TJs2LB861vfatzPhAkTcscdd+SOO+7Y4K8LAACaUouI\n97W55JJLcuSRR2bIkCGpVCo5+eST87vf/S7HH398jj/++LUG/wc+8IE89NBDqaurS01NTerr6zN7\n9uxmfBUAANB0WuxlMz169MjPf/7z1NbWpq6uLt/5zneyxx57vOVjampqMnjw4Nx///1Jkvvuuy87\n77zzhhgXAADWuxYb76eeemo++tGP5pJLLsknP/nJnH322Xnttdfe9nH77LNPpk+fniS55ZZbsu++\n+67vUQEAYINosfE+c+bMDB8+PNddd13uuuuutGvXLpdddtnbPm7gwIF57LHHsnjx4ixZsiQ9e/bc\nANMCAMD612LjfdKkSY2Xv7Rv3z59+vRJmzZt3vZxVVVV2W233TJ27Ni3vcwGAABK0mJ+YPX+++/P\nQQcd1Hh70qRJufDCC3PxxRenTZs26dWrV8aOHfuO9rXvvvvmi1/8YsaNG7eepgUAgA2vqlKpVJp7\niJbgmpmvNvcIAMC7dPv02uYeAd6168ft/p4f22IvmwEAAFYn3gEAoBDiHQAACiHeAQCgEOIdAAAK\nId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHeAQCgEOIdAAAKId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHe\nAQCgEOIdAAAKId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHeAQCgEOIdAAAKId4BAKAQ4h0AAAoh3gEA\noBDiHQAACiHeAQCgEOIdAAAKId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHeAQCgEOIdAAAKId4BAKAQ\n4h0AAAoh3gEAoBDiHQAACiHeAQCgEOIdAAAKId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHeAQCgEOId\nAAAKId4BAKAQ4h0AAAoh3gEAoBDiHQAACiHeAQCgEOIdAAAKUVWpVCrNPURLUFtbm4EDBzb3GLQw\n1gVrY12wNtYFa2Nd0NSceQcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAA\nKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiE\neAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAoRFWl\nUqk09xAtwTUzX23uESjc7dNrm3sEAJrR9eN2X+NrtbW1GThwYDNMw/uVM+8AAFAI8Q4AAIUQ7wAA\nUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI\n8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEO\nAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAA\nhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ\n7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFeEfx\nfuWVV2b48OE59thjc9xxx2X27Nk5//zzs2DBgvf8xGeddVbuueeef7j9qKOOype+9KUcddRROeKI\nI7Lvvvvm7rvvbnzsySefvNr9d9lllyTJtGnT8pnPfCZLly5t3HbqqafmwQcffM+zAgBAS1Dzdnd4\n6qmncscdd+SnP/1pqqqq8vjjj2fUqFH55S9/ud6Hu/DCC9O3b98kyTPPPJOvf/3r2W233ZIktbW1\nufnmm3PAAQes8bjly5dn4sSJmThx4nqfEQAANpS3jfcuXbpkwYIFmTp1aoYMGZIPf/jDmTp1ao46\n6qiMHTs206dPz9y5c7N48eK88sorOfzww/Ob3/wmzz77bC688MJsvvnmOeWUU9K1a9e89NJLGTJk\nSE499dTG/b/xxhs577zzMnfu3DQ0NOQb3/hGPvGJT6wxx4IFC7LZZps13j7ttNNy6aWXZqeddsoW\nW2yx2n0POOCAPPzww7nzzjuz++67r8v3BwAAWoy3vWymS5cuufzyy/Nf//VfOfTQQ7Pnnnvmzjvv\nXO0+bdu2zVVXXZXPfe5zufvuu3PFFVfkxBNPzK233pokmT9/fi644IJMnTo1M2fOzKOPPtr42Jtu\nuimdO3fOddddl8suuyzjxo1r3DZq1KgMGzYsQ4YMyY033ph/+7d/a9zWrVu3nHLKKRkzZswaM1dX\nV+eCCy7IxIkTs3jx4nf/XQEAgBbobc+8z507Nx06dGgM5z/96U858cQTs/nmmzfeZ7vttkuSdOzY\nMf369UuSdOrUKStXrkyS9O/fPx/4wAeSJAMGDMizzz7b+Ngnn3wytbW1mTVrVpKkrq6uMbj/dtnM\nDTfckFtuuSVbbrnlarPtt99++c///M9cf/31a8y99dZb5+ijj863vvWtVFVVvcNvBwAAtFxve+Z9\nzpw5GTt2bGOI9+nTJx07dkx1dXXjfd4ujp9++uksX7489fX1mTVrVmPgJ8k222yTvffeO1OmTMkP\nf/jD7LnnnunUqdNqjx82bFi23HLLTJ48eY19jx07NldffXWWLVu2xrYjjzwyS5YsycyZM9/uZQIA\nQIv3tvH+uc99LoMHD87BBx+cYcOG5bjjjsuZZ56Zjh07vuMnad26dU455ZQcfPDBGTp0aPr379+4\nbdiwYXnmmWdy5JFHZtiwYenZs2datVpzrDFjxuTWW2/NE088sdrXu3TpkrPOOivLly9f4zFVVVWZ\nOHFiVq1a9Y5nBQCAlqqqUqlU1ucTPP/88xk5cmRuvPHG9fk06+yama829wgU7vbptc09AgDN6Ppx\na35IRm1tbQYOHNgM0/B+5Zc0AQBAIdZ7vPfq1avFn3UHAIASOPMOAACFEO8AAFAI8Q4AAIUQ7wAA\nUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI\n8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEO\nAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAA\nhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ\n7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIWoae4BWortW/85AwcObO4xaGFqa2vf8bo4\nZqfd1/M0tBTvZl2w8bAugA3BmXcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiE\neAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgH\nAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCA\nQlRVKpVKcw/RElwz89XmHgGA96nbp9c29whAC3L9uN3f82OdeQcAgEKIdwAAKIR4BwCAQoh3AAAo\nhHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4\nBwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcA\ngEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBC\niHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3\nAAAohHgHAIBCiHcAACiEeAcAgEKIdwAAKIR4BwCAQoh3AAAohHgHAIBCiHcAAChETXMP8FbmzZuX\nSZMm5cUXX0zbtm3Ttm3bnHHGGfn1r3+dW265Jd26dUuSLFmyJHvttVdGjBiRadOm5Zlnnsnpp5/e\nuJ9TTz01w4YNyyc+8YnmeikAALDOWmy8L1++PCNGjMj48eOzww47JElmzZqVcePGZfDgwRk+fHgO\nO+ywJMmqVauy11575ZBDDmnOkQEAYL1qsZfN3Hnnndlpp50awz1JBgwYkJ/85Cdr3Hfx4sWpq6vL\nJptssiFHBACADarFnnl//vnn07t378bbI0aMyNKlS7Nw4cIMGjQot9xyS2699da88MIL6d69eyZM\nmJAOHTr8w/1VVVVtiLEBAGC9abFn3rfYYos8//zzjbcvv/zyTJkyJZ06dUp9fX2GDx+ea6+9Nt/9\n7nfz8ssvZ+utt06StG3bNqtWrVptX6+//nratm27IccHAIAm12LjfejQoZkxY0YeeeSRxq/NnTs3\nL7744mpn0bfffvuccMIJGTlyZBoaGtK/f/888MADWbZsWZI3f5j1z3/+c/r27bvBXwMAADSlFnvZ\nTPv27XP55Zfn4osvzkUXXZS6urrU1NRk/PjxmTVr1mr3Pfjgg3Pbbbflpz/9aY444ogcfvjhOfzw\nw9O+ffvU1dVlzJgxad++fTO9EgAAaBpVlUql0txDtATXzHy1uUcA4H3q9um1zT0C0IJcP2739/zY\nFnvZDAAAsDrxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACF\nEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDv\nAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAA\nUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI8Q4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACFEO8AAFAI\n8Q4AAIUQ7wAAUAjxDgAAhRDvAABQiJrmHgAAaLk2aZ20qalq7jGgOKvqKln5RtPvV7wDAGu104c3\ny6cH9ky7tq2bexQozusr3shdtfMz8/FXm3S/4h0AWMMmrZNPD+yZDu3bNfcoUKQO7Vvn0wN75uGn\nXm3SM/CueQcA1tCmpsoZd1hHm7Zt3eSXnYl3AABYD9bHT4u4bAYAeEe++YPaJt3fhK8MfMvtP7x8\ncv785BNZvOi/s3LlimyxZc906tQ5Y8Ze8Lb7fvqpOZn5wD054ugT1rr9D79/IAsXvpi99jnoPc2e\nJPt+bud8+CMDVvvamaPHZ/Ou3d7zPpPkt7/+Veb95bkce+LX1mk/b+feeU27v09t9dbbW/r7Of2W\nafncnvtl7nNPv+VzNTfxDgC0SCeMODXJe4vZvv0+lL79PvQPtw8a/Ml1nq9jx83y7ck/WOf9bCxa\n+vv5s+t+nD0+t/fbPldzE+8AQFFmPVKbq6+8NDWtW+cLex+YNptsklt+cVMqlUqSZMzYCzP32adz\n66/+I2efMzHHHXVQttt+QJ6f95d07twlY8ZemDt+Oz3z/vJc9t7vi7lgwjfTtWv3vLDg+Wzb/yP5\n2qln5ZVXluTCCd/MG2+sSq+tPpg/PvyHXH3tz9/RfNf++MrMm/dcXlm8OK8tfTUjvnZGtv+Xj+WO\n/7wtN//HT9O6dZv07LlVvn7amNTX1+WSb4/LwpdeTH1dXUZ87fQkyROPz87oM07OK68szt77fXGd\nzii3dC3h/bx9+i+yeNF/54LxY3LAFw9rfK5jjzww231kQObPn5eP7jAory9dmjlPPJpeW30wZ4we\nl78ufDHfu2RiVq1alTZt2uTrI0ena7ct1uv3S7z/j+1b/zkDB771X9+x8amtrbUuWIN1wdq81bo4\nZqfdN/A0627VqlVJkjZt2qy359im52bv6H5dO2+a1xZv0nj/vz7fLqnUZdrUaUmSK664Ij/58VXZ\ndNNNc+6552beU49ky+7d02HT1tmm52Z58YX5uf66Kdlyyy0zbNiwLFs0t3GfW3XvkBcXzMt1U36c\nTTfdNHvssUc6tlmZG26+Nvvu/fkcccQRuf/++/OnRx5aY96lS1/LeWed1Hi7W7duufjii9N5s02y\nvHPH/PCK72fWrFn55je/mWuuuSY/u/ZH+fnPf54OHTpk4sSJ+f2901NXV5cP9ds6P7zi+3nyySfz\nwAMPpGvnzdKxfdtcffXVmT9/fk488cSc/JXhTfNN/z/unde0H2FY8vs54oSjctNP/19+cPmleeSR\nRxqfa+FLL+T666aka9euGTx4cG666ab07ds3Q4cOzeYdk+9NuiwnHv/l7LbbbpkxY0ZuvPYHufji\nixv3u2rVqlx62ieb9N8j8Q4AFKdPnz6Nf/6nf/qnjBo1Ku3bt88zzzyTj33sY6vdt3Pnztlyyy2T\nJFtuuWVWrly52vbevXunQ4cOSZKuXbtm5cqVefrpp3PggQcmSQYNGrTWGTp16pQpU6asddtOO+2U\nJOnbt29efvnlzJs3L/369Wt8no9//OO57777UqlUMmTIkCTJtttum2233TbTpk3Ldtttl6qqqnTt\n2jUrVqx4V9+bErWE93NtPvCBD6RHjx5Jknbt2qVfv35Jko4dO2blypV58skn84Mf/CA/+tGPUqlU\n0rr1+v+EJvEOABSnVas3PzDvtddey/e+973cddddSZIvf/nLjZdb/E1V1Vt/5sfatm+77bZ5+OGH\n8+EPfziPPPLIu57v0Ucfzf7775+nnnoq3bt3T69evfL000/n9ddfT7t27fL73/8+ffr0SVVVVf70\npz9ljz32yLx58/Kd73wnu+yyy9vO/H7TEt7PqqqqNDQ0vKvn2mabbXLsscdmxx13zNNPP52HHnro\nLe/fFMQ7AFCsDh06ZMcdd8yBBx6Ydu3aZbPNNsvChQvTq1evddrvCSeckDPPPDO33XZbunXrlpqa\nNZPplVdeyVFHHbXa10aOHJkkefzxx3PMMcdk6dKlGT9+fLp06ZKvfe1rOfroo9OqVav07t07p5/+\n5vXto0ePzpFHHpn6+vqMHj06f/7zn9dp9pI15/s5aNCgnHjiiTnppJPWsoe1GzVqVMaOHZuVK1dm\nxYoVGTNmzDrN+U5UVf7+f2c2Uq5hZW2sC9bGumBt3m/rYkNc896S3X333encuXMGDBiQBx544M3r\nsH/yk3f02EsvvTSbb755DjvssCxbtizt27dfz9Pydtbl/VwX6+PfI2feAQD+Tq9evTJ69OhUV1en\noaFhg5xRZf15P72f4h0A4O/07ds3P/vZz97TY7/2tfX7y5V499bl/WxpWjX3AAAA8H60Pq5OF+8A\nwBpatWqVurq65h4DilZfX9/4STpNxWUzAMAaampqsnz58rz++uuprq7e6D66sKm88cYbjT+0yMaj\nUqmkvr4+9fX1a/1km3XhzDsAsFYdO3ZMmzZthPs6ePrpp5t7BJpBVVVV2rRpk44dOzb5vp15BwD+\noaY+a7gx2lg/bpP1w5l3AAAohHgHAIBCiHcAAChEVWV9fAAlAADQ5Jx5BwCAQoh3AAAohHgHAIBC\niHcAACiEeAcAgEKIdwAAKMRG/zuPGxoaMnbs2MyZMydt2rTJhAkT8sEPfrC5x6KZHHDAAenYsWOS\npFevXjn00ENz/vnnp7q6OrvuumtOPvnkZp6QDemPf/xjLrrookyZMiVz587NWWedlaqqqvzzP/9z\nzjvvvLRq1Srf//73c9ddd6WmpiajR4/OgAEDmnts1rP/uy4effTRfPWrX83WW2+dJDnssMOy1157\nWRcbkTfeeCOjR4/O/Pnzs2rVqowYMSL9+vVzvNjIrW1dbLHFFk1zvKhs5G6//fbKqFGjKpVKpfLw\nww9XvvrVrzbzRDSXFStWVPbff//VvrbffvtV5s6dW2loaKgcf/zxldmzZzfTdGxoV155ZWWfffap\nHHzwwZVKpVL5yle+Upk5c2alUqlUzjnnnMpvfvObyuzZsytHHXVUpaGhoTJ//vzKQQcd1JwjswH8\n/bq48cYbK1ddddVq97EuNi5Tp06tTJgwoVKpVCqLFi2q7Lbbbo4XrHVdNNXxYqO/bKa2tjaf+tSn\nkiQf+9jHMnv27GaeiObyxBNPZPny5Tn22GNz9NFH56GHHsqqVavSu3fvVFVVZdddd82MGTOae0w2\nkN69e+fSSy9tvP3oo49m8ODBSZIhQ4bkgQceSG1tbXbddddUVVWlR48eqa+vz6JFi5prZDaAv18X\ns2fPzl133ZUjjjgio0ePztKlS62Ljcyee+6ZU045pfF2dXW14wVrXRdNdbzY6ON96dKl6dChQ+Pt\n6urq1NXVNeNENJe2bdvmuOOOy1VXXZVvfetbOfvss7Pppps2bm/fvn1ee+21ZpyQDenzn/98amr+\n98rCSqWSqqqqJP+7Fv7++GGNvP/9/boYMGBAzjzzzFx33XXZaqut8u///u/WxUamffv26dChQ5Yu\nXZqvf/3r+cY3vuF4wVrXRVMdLzb6eO/QoUOWLVvWeLuhoWG1AzMbjz59+mS//fZLVVVV+vTpk44d\nO2bJkiWN25ctW5bNNtusGSekObVq9b+Hy7+thb8/fixbtqzxZybYOHz2s5/N9ttv3/jnxx57zLrY\nCL3wwgs5+uijs//++2ffffd1vCDJmuuiqY4XG32877jjjrnnnnuSJI888ki23XbbZp6I5jJ16tRc\ncMEFSZKXXnopy5cvT7t27fKXv/wllUol9913XwYNGtTMU9Jctttuuzz44INJknvuuSeDBg3Kjjvu\nmPvuuy8NDQ1ZsGBBGhoa0qVLl2aelA3puOOOy6xZs5IkM2bMyEc+8hHrYiPz8ssv59hjj80ZZ5yR\nL33pS0kcL1j7umiq48VGf4r5s5/9bO6///4MGzYslUolEydObO6RaCZf+tKXcvbZZ+ewww5LVVVV\nJk6cmFatWuX0009PfX19dt1113z0ox9t7jFpJqNGjco555yTSy65JNtss00+//nPp7q6OoMGDcqh\nhx6ahoaGnHvuuc09JhvY2LFjM378+LRu3Tqbb755xo8fnw4dOlgXG5Errrgir776ai677LJcdtll\nSZIxY8ZkwoQJjhcbsbWti7POOisTJ05c5+NFVaVSqazvFwAAAKy7jf6yGQAAKIV4BwCAQoh3AAAo\nhHgHAIBCiHcAACjERv9RkQCleP7557Pnnnumb9++q339kEMOyRFHHLHO+3/wwQfz/e9/P1OmTHnH\nj7n55pvzyiuvZNq0aUne/KUk7dq1S6dOndKmTZvcdNNN2X///fOLX/xinedLkjPPPDOnnXZaunfv\nvsa2ZcuWZdSoUfnud7+b6urqJnk+gJZGvAMUpFu3bk0Wwk3h3nvvzTe+8Y0cc8wxSd78HOPBgwfn\noIMOarxPU8175513pnv37msN9+TNXyu+884754YbbmiS/5kBaInEO8D7wM4775zPfvazefjhh9O+\nfftcdNFF6dWrVx555JGcf/75WblyZTp37pxx48blgx/8YB5//PGce+65WbFiRTp16pSLLrooSbJo\n0aKccMIJ+ctf/pI+ffrke9/7XlatWpWRI0fm5ZdfTpKcdNJJGTp0aONvBNxqq63ecrYPfehDmTNn\nTi699NIsWLAgzz33XBYtWpQRI0ZkxowZ+eMf/5j+/ftn8uTJqaqqypVXXpnbbrut8ZejnXHGGamq\nqsqPfvSjjBs3Lsmbv51w0qRJSZJOnTrl4osvTpcuXbLPPvvkkEMOyeGHH56qqqr1+B0HaB6ueQco\nyMKFC7P//vuv9s+cOXOyaNGi7LDDDvnVr36VvffeOxMmTGiM7nPOOSe//OUvM2zYsIwcOTJJcvrp\np+df//Vf86tf/Sp77bVXrrnmmiTJggULcu655+a2227Lyy+/nAceeCC//e1v07Nnz0ybNi3nn39+\n/vCHPyRJZs2alX/5l395V/OuTazSAAADuElEQVQ/+eSTmTJlSsaPH5+zzz47J5xwQm655ZY89thj\nmTNnTu65557Mnj07U6dOzc0335yXXnopv/zlL7NkyZI899xzjZcMXXbZZRk7dmymTZuWT37yk3ns\nsceSvBny7dq1y5w5c5rqWw7QojjzDlCQf3TZzCabbJIDDjggSXLggQfmkksuyXPPPZfNNtssAwYM\nSJJ84QtfyLnnnpv58+fnr3/9a3bfffckyeGHH57kzWve+/fv33gmvW/fvlm8eHF22GGHXHLJJXnp\npZfy6U9/OieddFKS5J577smQIUPe1fy77LJLampq0qNHj3Tt2jX9+vVLknTv3j2vvPJKZsyYkVmz\nZjVedrNixYr06NEjffr0Sbdu3Rr3M3To0Jx88snZY489MnTo0Oyyyy6N23r06JHnnnsu/fv3f1ez\nAZRAvAO8D7Rq1arxMpGGhoZUV1enoaFhjftVKpUkWe2SkpUrV2bhwoVJkpqa//3PQlVVVSqVSrbe\neuvcdtttuffee3PnnXfm6quvzvTp0/PQQw/lK1/5yruas3Xr1o1//r/P9Tf19fU55phj8uUvfzlJ\n8uqrr6a6ujrPPPPMavcfPnx4dt9999x5552ZNGlSZs2alREjRiRJqqur06qVv1gG3p8c3QDeB5Yv\nX5477rgjSTJt2rQMGTIk22yzTZYsWZJZs2YlSaZPn54ePXqkZ8+e6d69e+67774kb/5A6Xe/+91/\nuO9rr702l156ab7whS/kvPPOy6JFi7JkyZK0b98+m2yySZO+jp122im/+MUvsmzZstTV1eWkk07K\n7bffnq222iovvPBC4/0OPvjgLFu2LMOHD8/w4cMbL5tJkvnz56d3795NOhdAS+HMO0BB/nbN+//1\n8Y9/PEny61//OpMnT063bt1y4YUXpk2bNpk8eXLGjx+f5cuXp1OnTpk8eXKSZNKkSRk7dmwmTZqU\nzp0759vf/naeffbZtT7nAQcckJEjR2bfffdNdXV1zjjjjNx7773Zddddm/z1feYzn8kTTzyRQw45\nJPX19fnUpz6VAw88MFVVVendu3eeeuqp9OvXLyNHjsxZZ52VmpqatGvXLhMmTEjy5pn6pUuXumQG\neN+qqvzt71ABKNbfPtHl/ex3v/td/vCHP2TUqFH/8D7XXHNNampqfFQk8L7lshkAijB06NAsXLgw\nL7300lq3L1u2LDNmzMihhx66gScD2HCceQcAgEI48w4AAIUQ7wAAUAjxDgAAhRDvAABQCPEOAACF\nEO8AAFCI/w/wUUTBCEw+OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f5e493630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=[LSTM_steps, SRNN_steps, GRU_steps], y=[\"LSTM\", \"SimpleRNN\", \"GRU\"], label=\"Training Epoch\", color=\"b\")\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=[t1, t2, t3] , y=[\"LSTM\", \"SimpleRNN\", \"GRU\"], label=\"Training time\", color=\"b\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(xlim=(0, 250), ylabel=\"\",\n",
    "       xlabel=\"Epochs/Time(s)\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.savefig(\"barplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mean_squared_error', 'loss', 'mean_squared_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAKqCAYAAAAufAhdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f5e24d198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "key_ = list(history.history.keys())[3]\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(history.history[key_], label=\"LSTM\")\n",
    "plt.plot(history2.history[key_], label=\"SimpleRNN\")\n",
    "plt.plot(history3.history[key_], label=\"GRU\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that GRU and LSTM perform better than a Simple RNN. LSTM is also performing slightly better that GRU but require more computation time. We can also check the output and compare it to the real output provided by the graph (see the y description in preparation of data section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :\n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 1 0 0]\n",
      "  [0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]]\n",
      "\n",
      "\n",
      "LSTM predicts :\n",
      "[[[ 0.02   0.556  0.009 -0.018  0.444 -0.007 -0.007]\n",
      "  [ 0.048  0.702  0.006  0.099 -0.013  0.113  0.   ]\n",
      "  [ 0.007  0.711  0.033  0.073  0.003  0.228 -0.   ]\n",
      "  [-0.001  0.694  0.007  0.07   0.021  0.274 -0.   ]\n",
      "  [ 0.005  0.038  0.001  0.     0.726  0.079  0.022]\n",
      "  [-0.04   0.034  0.202  0.821 -0.016  0.042  0.032]\n",
      "  [-0.     0.625 -0.009  0.006 -0.003  0.423  0.014]\n",
      "  [-0.003 -0.012 -0.002 -0.     0.702  0.284  0.077]\n",
      "  [-0.053 -0.002  0.254  0.653 -0.028  0.063  0.027]\n",
      "  [ 0.029  0.026  0.043  0.009  0.002  0.026  0.894]\n",
      "  [ 0.011  0.007 -0.001 -0.008  0.     0.011 -0.003]\n",
      "  [ 0.007 -0.001  0.001 -0.001  0.003  0.016 -0.002]\n",
      "  [ 0.005 -0.009  0.002 -0.002  0.001  0.011  0.001]\n",
      "  [ 0.003 -0.01   0.004 -0.002 -0.     0.008  0.001]\n",
      "  [ 0.002 -0.009  0.004 -0.002 -0.     0.007  0.001]\n",
      "  [ 0.001 -0.008  0.004 -0.002 -0.     0.006  0.   ]\n",
      "  [ 0.001 -0.007  0.003 -0.002  0.     0.005  0.   ]\n",
      "  [ 0.    -0.006  0.003 -0.002  0.001  0.005  0.   ]\n",
      "  [ 0.    -0.006  0.002 -0.002  0.001  0.005  0.001]\n",
      "  [ 0.    -0.005  0.002 -0.002  0.001  0.005  0.001]]]\n",
      "\n",
      "\n",
      "GRU predicts :\n",
      "[[[-0.051  0.586 -0.087 -0.012  0.414 -0.04   0.006]\n",
      "  [-0.028  0.672  0.138  0.127 -0.013  0.277 -0.044]\n",
      "  [-0.046  0.623  0.078  0.064  0.011  0.31  -0.037]\n",
      "  [-0.037  0.602  0.1    0.055  0.009  0.338 -0.031]\n",
      "  [ 0.041  0.005  0.056  0.037  0.779  0.232 -0.061]\n",
      "  [ 0.07   0.005  0.164  0.782 -0.069  0.03   0.002]\n",
      "  [-0.018  0.571  0.02  -0.028 -0.005  0.375  0.004]\n",
      "  [ 0.056 -0.001  0.01  -0.034  0.725  0.291 -0.033]\n",
      "  [ 0.082 -0.001  0.153  0.704 -0.037  0.077 -0.015]\n",
      "  [ 0.027  0.103  0.078  0.007 -0.046  0.034  0.869]\n",
      "  [ 0.006  0.035  0.016 -0.018  0.001  0.016  0.015]\n",
      "  [ 0.006  0.015  0.011 -0.005  0.008  0.012 -0.007]\n",
      "  [ 0.006  0.008  0.005  0.001  0.006  0.006 -0.003]\n",
      "  [ 0.005  0.005  0.003 -0.     0.005  0.003 -0.001]\n",
      "  [ 0.005  0.003  0.003 -0.001  0.005  0.002 -0.001]\n",
      "  [ 0.005  0.002  0.004 -0.001  0.005  0.001 -0.003]\n",
      "  [ 0.005  0.002  0.004 -0.001  0.005  0.001 -0.004]\n",
      "  [ 0.005  0.001  0.004 -0.     0.005  0.001 -0.004]\n",
      "  [ 0.005  0.001  0.004 -0.     0.005  0.    -0.004]\n",
      "  [ 0.005  0.001  0.004 -0.     0.005  0.    -0.004]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input :\")\n",
    "print(X_val)\n",
    "print(\"\\n\\nLSTM predicts :\")\n",
    "y_pred = model.predict(X_val)\n",
    "print(y_pred)\n",
    "print(\"\\n\\nGRU predicts :\")\n",
    "y_pred = model3.predict(X_val)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply the output by removing small output and compare it to the possible output (we will only keep prediction from GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred < 0.1, 0, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.     0.586  0.     0.     0.414  0.     0.   ] \t [0 1 0 0 1 0 0]\n",
      "[ 0.     0.672  0.138  0.127  0.     0.277  0.   ] \t [0 1 0 0 0 1 0]\n",
      "[ 0.     0.623  0.     0.     0.     0.31   0.   ] \t [0 1 0 0 0 1 0]\n",
      "[ 0.     0.602  0.1    0.     0.     0.338  0.   ] \t [0 1 0 0 0 1 0]\n",
      "[ 0.     0.     0.     0.     0.779  0.232  0.   ] \t [0 0 0 0 1 1 0]\n",
      "[ 0.     0.     0.164  0.782  0.     0.     0.   ] \t [0 0 1 1 0 0 0]\n",
      "[ 0.     0.571  0.     0.     0.     0.375  0.   ] \t [0 1 0 0 0 1 0]\n",
      "[ 0.     0.     0.     0.     0.725  0.291  0.   ] \t [0 0 0 0 1 1 0]\n",
      "[ 0.     0.     0.153  0.704  0.     0.     0.   ] \t [0 0 1 1 0 0 0]\n",
      "[ 0.     0.103  0.     0.     0.     0.     0.869] \t [0 0 0 0 0 0 1]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.] \t [0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for pred, real in zip(y_pred[0], y_possible[0]):\n",
    "    print(pred, \"\\t\", real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah !! Output is balanced between both offset but with different \"probabilities\". We can also check how well they are to generate sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it as generator\n",
    "\n",
    "As explained previously, we trained our model as a many-to-many RNN. Now we want a generator so we are going to use a one-to-many model but reusing knowledge from the training. \n",
    "\n",
    "Before that, we will need an evaluation function which take the output, pick the next input based on the probability to have this output, create the next input and run it until the graph is over. After that, we will check is the created word is really a Reber word. This will be done with following functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pick_From_Output(x):\n",
    "    y = np.zeros_like(x)\n",
    "    x = np.where(x < 0.1, 0, x)\n",
    "    x = x[0]/x[0].sum(axis=1)\n",
    "    i = np.random.choice(list(range(7)), size=1, p=x[0])\n",
    "    y[0,0,i] = 1\n",
    "    return y\n",
    "\n",
    "def evaluate(model, nb_word = 1, max_iter = 50):\n",
    "    good_pred = 0\n",
    "    for _ in range(nb_word):\n",
    "        model.reset_states()\n",
    "        first_input = np.array([[[1,0,0,0,0,0,0]]])\n",
    "        word = \"B\"\n",
    "        loop = 0\n",
    "        nextLetter = \"B\"\n",
    "        next_seq = first_input\n",
    "        while nextLetter != \"E\" and loop < max_iter:\n",
    "            y_pred = model.predict(next_seq)\n",
    "            next_seq = Pick_From_Output(y_pred)\n",
    "            nextLetter = reber.sequenceToWord(next_seq[0])\n",
    "            loop += 1\n",
    "            word += nextLetter\n",
    "        if reber.in_grammar(word):\n",
    "            good_pred += 1\n",
    "    acc = 100*good_pred/nb_word\n",
    "    print(\"Good prediction : {:.2f}%\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_samples = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create both model as one-to-many and evaluate them 20 times on 100 words generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model = load_model(\"lstm_simple.h5\")  # lstm_simple /  srnn_simple / gru_simple\n",
    "\n",
    "newModel = Sequential()\n",
    "newModel.add(LSTM(units=7, stateful=True, batch_input_shape=(1,1,7), return_sequences=True))\n",
    "newModel.set_weights(Model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction : 90.00%\n",
      "Good prediction : 92.00%\n",
      "Good prediction : 94.00%\n",
      "Good prediction : 95.00%\n",
      "Good prediction : 96.00%\n",
      "Good prediction : 94.00%\n",
      "Good prediction : 95.00%\n",
      "Good prediction : 94.00%\n",
      "Good prediction : 93.00%\n",
      "Good prediction : 98.00%\n",
      "Good prediction : 93.00%\n",
      "Good prediction : 95.00%\n",
      "Good prediction : 92.00%\n",
      "Good prediction : 93.00%\n",
      "Good prediction : 99.00%\n",
      "Good prediction : 94.00%\n",
      "Good prediction : 97.00%\n",
      "Good prediction : 95.00%\n",
      "Good prediction : 96.00%\n",
      "Good prediction : 93.00%\n"
     ]
    }
   ],
   "source": [
    "result_LSTM = []\n",
    "for _ in range(nb_samples):\n",
    "    result_LSTM.append(evaluate(newModel, 100, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model = load_model(\"srnn_simple.h5\")\n",
    "\n",
    "newModel = Sequential()\n",
    "newModel.add(SimpleRNN(units=7, stateful=True, batch_input_shape=(1,1,7), return_sequences=True))\n",
    "newModel.set_weights(Model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction : 11.00%\n",
      "Good prediction : 8.00%\n",
      "Good prediction : 8.00%\n",
      "Good prediction : 12.00%\n",
      "Good prediction : 10.00%\n",
      "Good prediction : 9.00%\n",
      "Good prediction : 10.00%\n",
      "Good prediction : 13.00%\n",
      "Good prediction : 6.00%\n",
      "Good prediction : 7.00%\n",
      "Good prediction : 12.00%\n",
      "Good prediction : 14.00%\n",
      "Good prediction : 8.00%\n",
      "Good prediction : 14.00%\n",
      "Good prediction : 6.00%\n",
      "Good prediction : 16.00%\n",
      "Good prediction : 8.00%\n",
      "Good prediction : 7.00%\n",
      "Good prediction : 12.00%\n",
      "Good prediction : 11.00%\n"
     ]
    }
   ],
   "source": [
    "result_SRNN = []\n",
    "for _ in range(nb_samples):\n",
    "    result_SRNN.append(evaluate(newModel, 100, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = load_model(\"gru_simple.h5\")\n",
    "\n",
    "newModel = Sequential()\n",
    "newModel.add(GRU(units=7, stateful=True, batch_input_shape=(1,1,7), return_sequences=True))\n",
    "newModel.set_weights(Model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good prediction : 89.00%\n",
      "Good prediction : 87.00%\n",
      "Good prediction : 81.00%\n",
      "Good prediction : 84.00%\n",
      "Good prediction : 82.00%\n",
      "Good prediction : 84.00%\n",
      "Good prediction : 88.00%\n",
      "Good prediction : 86.00%\n",
      "Good prediction : 83.00%\n",
      "Good prediction : 87.00%\n",
      "Good prediction : 76.00%\n",
      "Good prediction : 87.00%\n",
      "Good prediction : 88.00%\n",
      "Good prediction : 77.00%\n",
      "Good prediction : 86.00%\n",
      "Good prediction : 86.00%\n",
      "Good prediction : 86.00%\n",
      "Good prediction : 82.00%\n",
      "Good prediction : 87.00%\n",
      "Good prediction : 87.00%\n"
     ]
    }
   ],
   "source": [
    "result_GRU = []\n",
    "for _ in range(nb_samples):\n",
    "    result_GRU.append(evaluate(newModel, 100, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'LSTM': result_LSTM, 'Simple RNN': result_SRNN, 'GRU' : result_GRU}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18f705c5940>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAD3CAYAAAC6jVe2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f705c26d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[\"LSTM\", \"Simple RNN\", \"GRU\"], data=df, capsize=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that bost LSTM and GRU outperform the standard RNN. In average LSTM is slightly better than GRU but takes also more time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAucAAAFkCAYAAACDy5UjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f707916d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax = sns.boxplot(x=[result_LSTM, result_SRNN, result_GRU], y=[\"LSTM\", \"SimpleRNN\", \"GRU\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz0AAAKqCAYAAAAQfDA6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f709444e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(nb_samples))\n",
    "y = [result_LSTM, result_SRNN, result_GRU]\n",
    "labels = [\"LSTM\", \"SimpleRNN\", \"GRU\"]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for y_arr, label in zip(y, labels):\n",
    "    plt.plot(x, y_arr, label=label)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workbook, we started to go through RNN. We check a simple model of both LSTM, GRU and SimpleRNN to check how fast and well they learn. On this example GRU and LSTM outperform the standard RNN due to the memory function. There is also a difference between LSTM and GRU but with slightly more epochs, they both perform similar. We can probably have better result by using a more advanced model but for such a simple model, we can see that it works really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "On a future notebook, we will explore Embedded Reber but using deeper RNNs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
